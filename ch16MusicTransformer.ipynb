{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "653089c9",
   "metadata": {},
   "source": [
    "# Chapter 16: Bulid and Train A Music Transformer\n",
    "\n",
    "\n",
    "In the previous chapter, you used MuseGAN to generate music that can pass as a real piece of multi-track music. The model treats a piece of music as a multi-dimensional object (similar to an image). The generator first creates a whole piece of music and presents to the critic to obtain feedback. Based on the evaluation from the critic, the generator gradually fine tunes the music piece until it can pass as a piece of music that is indistinguishable from real music in the training set. \n",
    "\n",
    "In this chapter, we'll create music with a different approach: we'll treat music as a sequence. We'll then use the techniques we learned in text generation from Chapter 14. In particular, we'll create a transformer to predict the most likely next element in a sequence based on the elements before it. The transformer is able to generate realistic-sounding music. \n",
    "\n",
    "It's a good place to end the book at this point since you have learned the ability to generalize different generative models. You can generalize the idea behind generative adversarial networks (GANs) to create patterns, figures, images, and now pieces of music. You can also generalize the idea behind ChatGPT-style transformers to predict the next element in a sequence. The technique can be used to generate text that can pass as human written. Now you also use the exact same model architechture to generate pieces of music that sound like real human-created music. You are ready to deploy these state-of-the-art generative models to your own projects. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d1c19a",
   "metadata": {},
   "source": [
    "Start a new cell in ch16.ipynb and execute the following lines of code in it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "117477d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.makedirs(\"files/ch16\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c268aecc",
   "metadata": {},
   "source": [
    "# 1. Music Files as Sequences\n",
    "Instead of treating a piece of music as a multi-dimensional object, we'll treat it as a sequence, similar to the text documents we have dealt with in Chapters 11-14. The idea behind music transformer models is first proposed by Huang et al in 2018 (https://arxiv.org/abs/1809.04281).\n",
    "\n",
    "In this section, we'll first download the training data and learn how to convert music files to sequences so that we can feed them to the a music transformer. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed84eadd",
   "metadata": {},
   "source": [
    "## 1.1. Downlaod the Music Files\n",
    "We'll download the piano performance from the MAESTRO dataset. Go to https://storage.googleapis.com/magentadata/datasets/maestro/v2.0.0/maestro-v2.0.0-midi.zip and download the zip file. Unzip it and place the folder /maestro-v2.0.0/ insider the folder /Desktop/ai/files/ch16/ on your computer. \n",
    "\n",
    "Make sure that there are four files (one of them is named \"maestro-v2.0.0.json\") plus ten subfolders insider the folder /maestro-v2.0.0/. Each of the ten subfolders contains more than 100 midi files. Open some midi files to get an idea on what the music pieces in the training data sound like. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d073ca",
   "metadata": {},
   "source": [
    "Next, we'll divide the midi files into train, validation, and test subsets. We first create three subfolders in /files/ch16/ as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c57af8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"files/ch16/train\", exist_ok=True)\n",
    "os.makedirs(\"files/ch16/val\", exist_ok=True)\n",
    "os.makedirs(\"files/ch16/test\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7f5710",
   "metadata": {},
   "source": [
    "Download the two files *ch16util.py* and *processor.py* from the book's GitHub repository and place them in /Desktop/ai/utils/ on your computer. We'll use the module *processor.py* to process midi files and the module is copied directly from Kevin Yang's GitHub repository  (https://github.com/jason9693/midi-neural-processor). The way we handle music files in this chapter has also benefited from the GitHub repository by Damon Gwinn (https://github.com/gwinndr/MusicTransformer-Pytorch). We focus mainly on building our own transformer and apply it to music generation since that's our main goal. \n",
    "\n",
    "The file \"maestro-v2.0.0.json\" in the folder /maestro-v2.0.0/ has all the midi file names and whether a file should go to the train, validation, or test subfolder. We'll group the midi files into three subfolders accordingly, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d463840",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "from utils.processor import encode_midi\n",
    "\n",
    "file=\"files/ch16/maestro-v2.0.0/maestro-v2.0.0.json\"\n",
    "maestro_json=json.load(open(file,\"r\"))\n",
    "for x in maestro_json:\n",
    "    mid=rf'files/ch16/maestro-v2.0.0/{x[\"midi_filename\"]}'\n",
    "    split_type = x[\"split\"]\n",
    "    f_name = mid.split(\"/\")[-1] + \".pickle\"\n",
    "    if(split_type == \"train\"):\n",
    "        o_file = rf'files/ch16/train/{f_name}'\n",
    "    elif(split_type == \"validation\"):\n",
    "        o_file = rf'files/ch16/val/{f_name}'\n",
    "    elif(split_type == \"test\"):\n",
    "        o_file = rf'files/ch16/test/{f_name}'\n",
    "    prepped = encode_midi(mid)\n",
    "    o_stream = open(o_file, \"wb\")\n",
    "    pickle.dump(prepped, o_stream)\n",
    "    o_stream.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce10d036",
   "metadata": {},
   "source": [
    "You can check the number of music files in the train, validation, and test subsets as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0db886ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 967 files in the train set\n",
      "there are 137 files in the validation set\n",
      "there are 178 files in the test set\n"
     ]
    }
   ],
   "source": [
    "train_size=len(os.listdir('files/ch16/train'))\n",
    "print(f\"there are {train_size} files in the train set\")\n",
    "val_size=len(os.listdir('files/ch16/val'))\n",
    "print(f\"there are {val_size} files in the validation set\")\n",
    "test_size=len(os.listdir('files/ch16/test'))\n",
    "print(f\"there are {test_size} files in the test set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f66583",
   "metadata": {},
   "source": [
    "Results show that there are 967, 137, and 178 pieces of music in the train, validation, and test subsets, respectively. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55202435",
   "metadata": {},
   "source": [
    "## 1.2. Prepare the Data for Training\n",
    "We'll use the *Data()* class we defined in the local module *ch16util.py* to create three datasets, *train*, *val*, and *test*, respectively, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c56ff65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.ch16util import Data\n",
    "\n",
    "train=Data('files/ch16/train')\n",
    "val=Data('files/ch16/val')\n",
    "test=Data('files/ch16/test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e0b2e3",
   "metadata": {},
   "source": [
    "In the the *Data()* class, we used the module *processor.py* that you just downloaded to encode midi files into sequences of numbers and saved them as pickle files in the three subfolders: *train*, *val*, and *test*. Let's print out a file from the validation subset and see what it looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4823d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2048])\n",
      "tensor([372,  67, 256,  ..., 258, 367,  57])\n"
     ]
    }
   ],
   "source": [
    "val1,_=val[0]\n",
    "print(val1.shape)\n",
    "print(val1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2549bd3",
   "metadata": {},
   "source": [
    "As you can see, the music piece is represented by a sequence of integers such as 372, 67, and so on. Let's use the module *processor.py* to decode the sequence to a midi file so that you can hear what it sounds like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c738e99f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "info removed pitch: 76\n",
      "info removed pitch: 64\n",
      "info removed pitch: 74\n",
      "info removed pitch: 62\n",
      "info removed pitch: 60\n",
      "info removed pitch: 72\n",
      "info removed pitch: 71\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pretty_midi.pretty_midi.PrettyMIDI at 0x273d0a5e510>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.processor import decode_midi\n",
    "\n",
    "file_path=\"files/ch16/val1.mid\"\n",
    "decode_midi(val1.cpu().numpy(), file_path=file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082c636c",
   "metadata": {},
   "source": [
    "Now go to the folder /files/ch16/ and open the file *val1.mid* with a music player and you should hear a short piece of piano music. Alternatively, you can run the following code cell and use the *music21* library to play it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01667af8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                <div id=\"midiPlayerDiv12618\"></div>\n",
       "                <link rel=\"stylesheet\" href=\"https://cuthbertLab.github.io/music21j/css/m21.css\">\n",
       "                \n",
       "                <script>\n",
       "                require.config({\n",
       "                    paths: {\n",
       "                        'music21': 'https://cuthbertLab.github.io/music21j/releases/music21.debug',\n",
       "                    }\n",
       "                });\n",
       "                require(['music21'], function(music21) {\n",
       "                    mp = new music21.miditools.MidiPlayer();\n",
       "                    mp.addPlayer(\"#midiPlayerDiv12618\");\n",
       "                    mp.base64Load(\"data:audio/midi;base64,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\");\n",
       "                });\n",
       "                </script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from music21 import midi\n",
    "\n",
    "mf = midi.MidiFile()\n",
    "mf.open(\"files/ch16/val1.mid\") \n",
    "mf.read()\n",
    "mf.close()\n",
    "stream = midi.translate.midiFileToStream(mf)\n",
    "stream.show('midi')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4220a2",
   "metadata": {},
   "source": [
    "Finally, we create data loader so that the data are in batches for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ff9ea02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size=2\n",
    "trainloader=DataLoader(train,batch_size=batch_size,\n",
    "                       shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02df292",
   "metadata": {},
   "source": [
    "# 2.  Create A Music Transformer\n",
    "In this section, we create a transformer for sequence prediction. To drive home the point that the state-of-the-art generative models can be applied to many different tasks in various fields, we'll use the exact same transformer that we used for text generation in Chapter 14. \n",
    "\n",
    "First we define some hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de6cc779",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# size of vocabulary (num of different music notes)\n",
    "ntoken = 390\n",
    "# embedding dimension\n",
    "d_model = 512\n",
    "# dimension of the feedforward network\n",
    "d_hid = 1024   \n",
    "# number of layers of encoder blocks\n",
    "nlayers = 6 \n",
    "# number of heads in multi-head self atttion\n",
    "nhead = 8  \n",
    "# drop out rate in dropout layers\n",
    "dropout = 0.1  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73036c27",
   "metadata": {},
   "source": [
    "## 2.1 Build the Transformer\n",
    "We use the standard positional encoding class that is defined by PyTorch. So we define the following *PositinalEncoding()* class in the file *ch16util.py*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc2f8827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# no need to run this cell, it's in the local module\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout = 0.1,\n",
    "                 max_len = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2)\\\n",
    "                     * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25216c46",
   "metadata": {},
   "source": [
    "The transformer we build is an encoder only transformer and has exactly the same architechture as the one we used in Chapter 14 when we predict text (what's the next word?). Here we are predicting what's the next music note, but the idea is the same. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3fa3af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# no need to run this cell, it's in the local module\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, ntoken, d_model, nhead, d_hid,\n",
    "                 nlayers, dropout=0.1):\n",
    "        super().__init__() \n",
    "        self.model_type=\"Transformer\"\n",
    "        self.pos_encoder=PositionalEncoding(d_model,dropout)\n",
    "        encoder_layers = TransformerEncoderLayer(d_model,\n",
    "                                 nhead, d_hid, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(\n",
    "            encoder_layers, nlayers)\n",
    "        self.embedding=nn.Embedding(ntoken,d_model)\n",
    "        self.d_model=d_model\n",
    "        self.linear=nn.Linear(d_model,ntoken)\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.embedding.weight.data.uniform_(-initrange,initrange)\n",
    "        self.linear.bias.data.zero_()\n",
    "        self.linear.weight.data.uniform_(-initrange, initrange)        \n",
    "\n",
    "    def forward(self,src):\n",
    "        mask = nn.Transformer.generate_square_subsequent_mask(\n",
    "            src.shape[1])        \n",
    "        src=self.embedding(src)*math.sqrt(self.d_model)\n",
    "        src=self.pos_encoder(src)\n",
    "        output=self.transformer_encoder(src,mask)\n",
    "        output=self.linear(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea69e171",
   "metadata": {},
   "source": [
    "We'll instanstiate a music transformer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8f611561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (pos_encoder): PositionalEncoding(\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (transformer_encoder): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-5): 6 x TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (embedding): Embedding(390, 512)\n",
      "  (linear): Linear(in_features=512, out_features=390, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from utils.ch16util import Model\n",
    "\n",
    "model=Model(ntoken,d_model,nhead,d_hid,nlayers,dropout)\n",
    "model=model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad7be94",
   "metadata": {},
   "source": [
    "Next, we define the opimizer and the loss function as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "78d56b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=0.0001\n",
    "opt=torch.optim.Adam(model.parameters(), lr=lr,\n",
    "           betas=(0.9,0.98), eps=10e-9)\n",
    "# ignore the padding token\n",
    "loss_func=torch.nn.CrossEntropyLoss(ignore_index=389)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e27a43",
   "metadata": {},
   "source": [
    "# 3. Train the Music Transformer\n",
    "We'll train the model for 50 epochs. The training procedure is similar to the one we used in previous chatpers, in particular, in Chapter 14 when we train a transformer for text generation.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5f7a9a50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 Train loss: 2073.9585721492767\n",
      "epoch 2 Train loss: 1984.5666620731354\n",
      "epoch 3 Train loss: 1975.7429637908936\n",
      "epoch 4 Train loss: 1973.3791658878326\n",
      "epoch 5 Train loss: 1969.632961511612\n",
      "epoch 6 Train loss: 1969.3244071006775\n",
      "epoch 7 Train loss: 1965.426896572113\n",
      "epoch 8 Train loss: 1965.7658879756927\n",
      "epoch 9 Train loss: 1961.5988173484802\n",
      "epoch 10 Train loss: 1961.1398780345917\n",
      "epoch 11 Train loss: 1959.468116760254\n",
      "epoch 12 Train loss: 1956.1517736911774\n",
      "epoch 13 Train loss: 1958.242532491684\n",
      "epoch 14 Train loss: 1958.210488319397\n",
      "epoch 15 Train loss: 1955.5550963878632\n",
      "epoch 16 Train loss: 1953.7387971878052\n",
      "epoch 17 Train loss: 1954.348418712616\n",
      "epoch 18 Train loss: 1954.9378719329834\n",
      "epoch 19 Train loss: 1955.8498182296753\n",
      "epoch 20 Train loss: 1954.2265331745148\n",
      "epoch 21 Train loss: 1952.2702848911285\n",
      "epoch 22 Train loss: 1952.1706442832947\n",
      "epoch 23 Train loss: 1953.2447504997253\n",
      "epoch 24 Train loss: 1950.514800310135\n",
      "epoch 25 Train loss: 1956.468938112259\n",
      "epoch 26 Train loss: 1951.0483691692352\n",
      "epoch 27 Train loss: 1950.5648379325867\n",
      "epoch 28 Train loss: 1951.5613141059875\n",
      "epoch 29 Train loss: 1947.0131151676178\n",
      "epoch 30 Train loss: 1951.8472402095795\n",
      "epoch 31 Train loss: 1949.2457761764526\n",
      "epoch 32 Train loss: 1950.2132377624512\n",
      "epoch 33 Train loss: 1950.5771868228912\n",
      "epoch 34 Train loss: 1952.0164766311646\n",
      "epoch 35 Train loss: 1950.1191799640656\n",
      "epoch 36 Train loss: 1949.34113407135\n",
      "epoch 37 Train loss: 1946.5327780246735\n",
      "epoch 38 Train loss: 1951.6182391643524\n",
      "epoch 39 Train loss: 1948.229867696762\n",
      "epoch 40 Train loss: 1947.5393397808075\n",
      "epoch 41 Train loss: 1948.146684885025\n",
      "epoch 42 Train loss: 1948.8479928970337\n",
      "epoch 43 Train loss: 1945.1352715492249\n",
      "epoch 44 Train loss: 1948.4646995067596\n",
      "epoch 45 Train loss: 1950.9781017303467\n",
      "epoch 46 Train loss: 1948.3322734832764\n",
      "epoch 47 Train loss: 1950.5551011562347\n",
      "epoch 48 Train loss: 1950.761726140976\n",
      "epoch 49 Train loss: 1947.913535118103\n",
      "epoch 50 Train loss: 1945.6228971481323\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for epoch in range(1,51):\n",
    "    tloss=0\n",
    "    for x,y in trainloader:\n",
    "        opt.zero_grad()\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        out = model(x)\n",
    "        out=out.reshape(out.shape[0]*out.shape[1],-1)\n",
    "        y = y.flatten()\n",
    "        loss = loss_func(out, y)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        tloss+=loss.item()\n",
    "    print(\"epoch\",epoch,\"Training loss:\",tloss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56d0e1a",
   "metadata": {},
   "source": [
    "The training takes anywhere from one hour to several hours, depending on your hardware and whether you use GPU-training. Once done, we save the trained weights for later use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0950ab49",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),\"files/ch16/musicTrans.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3f6f49",
   "metadata": {},
   "source": [
    "# 4. Music Generation with the Trained Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be3c1f4",
   "metadata": {},
   "source": [
    "We'll generate music with the trained transformer in this section. \n",
    "\n",
    "## 4.1. Create A Prompt\n",
    "We need a prompt so that the transformer can use it as inputs to predict the next music note. We'll pick a music piece from the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3c691166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "info removed pitch: 97\n",
      "info removed pitch: 66\n",
      "info removed pitch: 74\n",
      "info removed pitch: 53\n",
      "info removed pitch: 56\n",
      "info removed pitch: 65\n",
      "info removed pitch: 68\n",
      "info removed pitch: 80\n",
      "info removed pitch: 89\n",
      "info removed pitch: 46\n",
      "info removed pitch: 75\n",
      "info removed pitch: 58\n",
      "info removed pitch: 70\n",
      "info removed pitch: 87\n",
      "info removed pitch: 82\n",
      "info removed pitch: 94\n",
      "info removed pitch: 92\n",
      "info removed pitch: 96\n",
      "info removed pitch: 95\n",
      "info removed pitch: 61\n",
      "info removed pitch: 73\n",
      "info removed pitch: 45\n",
      "info removed pitch: 76\n",
      "info removed pitch: 88\n",
      "info removed pitch: 54\n",
      "info removed pitch: 57\n",
      "info removed pitch: 84\n",
      "info removed pitch: 62\n",
      "info removed pitch: 69\n",
      "info removed pitch: 78\n",
      "info removed pitch: 90\n",
      "info removed pitch: 77\n",
      "info removed pitch: 81\n",
      "info removed pitch: 93\n",
      "info removed pitch: 85\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pretty_midi.pretty_midi.PrettyMIDI at 0x2419547f810>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# picks the 43th song as the prompt\n",
    "prompt, _  = test[42]\n",
    "prompt = prompt.to(device)\n",
    "# keep only the first 250 notes\n",
    "len_prompt=250\n",
    "# Saving primer first\n",
    "from utils.processor import decode_midi\n",
    "file_path = \"files/ch16/prompt.mid\"\n",
    "decode_midi(prompt[:len_prompt].cpu().numpy(),\n",
    "            file_path=file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2ea52b",
   "metadata": {},
   "source": [
    "We randomly pick an index (42, in our case) and use it to retrieve a song in the test dataset. We keep only the first 250 notes so that we can later feed it to the trained model to predict the next music note. We save the prompt in the local folder. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b631b44b",
   "metadata": {},
   "source": [
    "## 4.2. A Function to Generate Music\n",
    "We'll create a *sample()* function, similar to what we defined in Chapter 14, to generate a sequence of a certain length. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ae48ec65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the softmax function for later use\n",
    "softmax=torch.nn.Softmax(dim=-1)\n",
    "def sample(prompt, seq_length=2000):\n",
    "    # create input to feed to the transformer\n",
    "    gen_seq = torch.full((1,seq_length), 389, \n",
    "                     dtype=torch.long).to(device)\n",
    "    idx = len(prompt)\n",
    "    gen_seq[..., :idx] = \\\n",
    "        prompt.type(torch.long).to(device)\n",
    "    while(idx < seq_length):\n",
    "        y = softmax(model(gen_seq\\\n",
    "                  [..., :idx]))[..., :388]\n",
    "        probs = y[:, idx-1, :]\n",
    "        distrib = torch.distributions.categorical.\\\n",
    "            Categorical(probs=probs)\n",
    "        next_token = distrib.sample()\n",
    "        gen_seq[:, idx] = next_token\n",
    "        if(next_token == 388):\n",
    "            break\n",
    "        idx += 1\n",
    "    return gen_seq[:, :idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af16d94d",
   "metadata": {},
   "source": [
    "The function takes the prompt as the input and uses the trained transformer to predict the most likely music note to follow the prompt. It then adds the predicted music note to the end of the prompt and use the new prompt as input to make predictions again. It keeps on doing this until the desired sequence length is reached (2000 music notes in our case). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea156a9e",
   "metadata": {},
   "source": [
    "## 4.3. Generate Music"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c76f86",
   "metadata": {},
   "source": [
    "First we load up the trained weights to the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1d1c647a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"files/ch16/musicTrans.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e258fb",
   "metadata": {},
   "source": [
    "We then call the *sample()* function to generate music: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9901a2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.processor import encode_midi\n",
    "\n",
    "file_path = \"files/ch16/prompt.mid\"\n",
    "prompt = torch.tensor(encode_midi(file_path))\n",
    "generated_music=sample(prompt, seq_length=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9d7544",
   "metadata": {},
   "source": [
    "Finally, we convert the generated music to the midi format, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0651a2cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "info removed pitch: 61\n",
      "info removed pitch: 62\n",
      "info removed pitch: 65\n",
      "info removed pitch: 33\n",
      "info removed pitch: 95\n",
      "info removed pitch: 84\n",
      "info removed pitch: 77\n",
      "info removed pitch: 39\n",
      "info removed pitch: 67\n",
      "info removed pitch: 51\n",
      "info removed pitch: 43\n",
      "info removed pitch: 52\n",
      "info removed pitch: 62\n",
      "info removed pitch: 85\n",
      "info removed pitch: 85\n",
      "info removed pitch: 51\n",
      "info removed pitch: 77\n",
      "info removed pitch: 77\n",
      "info removed pitch: 51\n",
      "info removed pitch: 40\n",
      "info removed pitch: 36\n",
      "info removed pitch: 40\n",
      "info removed pitch: 42\n",
      "info removed pitch: 53\n",
      "info removed pitch: 30\n",
      "info removed pitch: 40\n",
      "info removed pitch: 85\n",
      "info removed pitch: 85\n",
      "info removed pitch: 37\n",
      "info removed pitch: 37\n",
      "info removed pitch: 85\n",
      "info removed pitch: 36\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pretty_midi.pretty_midi.PrettyMIDI at 0x2420284d110>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#generated_music=generated_music.detach()\n",
    "music_data = generated_music[0].cpu().numpy()\n",
    "#music_data.write('midi', 'files/ch16/musicTrnas.mid')\n",
    "file_path = 'files/ch16/musicTrans.mid'\n",
    "decode_midi(music_data, file_path=file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01fa923c",
   "metadata": {},
   "source": [
    "You can listen to the gererated song like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5b904ef9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                <div id=\"midiPlayerDiv23891\"></div>\n",
       "                <link rel=\"stylesheet\" href=\"https://cuthbertLab.github.io/music21j/css/m21.css\">\n",
       "                \n",
       "                <script>\n",
       "                require.config({\n",
       "                    paths: {\n",
       "                        'music21': 'https://cuthbertLab.github.io/music21j/releases/music21.debug',\n",
       "                    }\n",
       "                });\n",
       "                require(['music21'], function(music21) {\n",
       "                    mp = new music21.miditools.MidiPlayer();\n",
       "                    mp.addPlayer(\"#midiPlayerDiv23891\");\n",
       "                    mp.base64Load(\"data:audio/midi;base64,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\");\n",
       "                });\n",
       "                </script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mf = midi.MidiFile()\n",
    "mf.open(\"files/ch16/musicTrans.mid\") \n",
    "mf.read()\n",
    "mf.close()\n",
    "stream = midi.translate.midiFileToStream(mf)\n",
    "stream.show('midi')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a679fb",
   "metadata": {},
   "source": [
    "Or you can listen to the music by pressing the play button below:\n",
    "\n",
    "<audio src=\"https://gattonweb.uky.edu/faculty/lium/ml/musicTrans.mp3\" type=\"audio/mpeg\" controls=\"\" controlsList=\"nodownload\"></audio>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

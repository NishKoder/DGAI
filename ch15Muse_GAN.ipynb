{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "653089c9",
   "metadata": {},
   "source": [
    "# Chapter 15: Music Generation with MuseGAN\n",
    "\n",
    "\n",
    "You'll learn to generate music in this chapter and the next. However, these two chapters take very different approaches. In this chapter, you'll use the techniques you learned in image GANs and treat a piece of music as a multi-dimensional object (similar to an image). Like all GAN models, there is a discriminator and a generator. The generator first creates a whole piece of music and presents to the discriminator. Based on the feedback from the discriminator, the generator gradually fine tunes the music piece until it can pass as a real piece of music from the training set. In contrast, we'll treat a piece of music as a sequence in the next chapter and use the techniques we learn in natural language processing to create music. \n",
    "\n",
    "The MuseGAN model is proposed by Dong, Hsiao, Yang, and Yang in 2017 (https://arxiv.org/abs/1709.06298). The code in this chapter is adapted from Azamat Kanametov's GitHub repository (https://github.com/akanametov/musegan). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d1c19a",
   "metadata": {},
   "source": [
    "Start a new cell in ch15.ipynb and execute the following lines of code in it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "117477d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.makedirs(\"files/ch15\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c268aecc",
   "metadata": {},
   "source": [
    "# 1. Multi-Track Music Files\n",
    "Multi-track music pieces have sounds from multiple instruments such as bass, drums, pianos, or strings. \n",
    "\n",
    "In this section, we'll first download the training data and learn how to convert music files to a format that the MuseGAN can understand. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed84eadd",
   "metadata": {},
   "source": [
    "## 1.1. Downlaod the Music Files\n",
    "We'll use JSB Chorales pianorolls data set from https://github.com/czhuang/JSB-Chorales-dataset. Download the music file *Jsb16thSeparated.npz* and place it in the folder /Desktop/ai/files/ch15/ on your computer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d073ca",
   "metadata": {},
   "source": [
    "Next, download the two utility modules *midi_util.py* and *MuseGAN_util.py* from the book's GitHub repository and place them in /Desktop/ai/utils/ on your computer. We can now load up the music files and organize them in batches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c57af8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from utils.midi_util import MidiDataset\n",
    "\n",
    "dataset = MidiDataset('files/ch15/Jsb16thSeparated.npz')\n",
    "loader = DataLoader(dataset, batch_size=64, \n",
    "                        shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7f5710",
   "metadata": {},
   "source": [
    "We can print out a song and see the data format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d463840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 2, 16, 84])\n"
     ]
    }
   ],
   "source": [
    "songs=next(iter(loader))\n",
    "first_song=songs[0]\n",
    "print(first_song.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce10d036",
   "metadata": {},
   "source": [
    "The shape of each song is (4, 2, 16, 84), meaning 4 tracks, 2 bars, 16 steps per bar, and 84 notes per step. All the values are normalized to the range between -1 and 1, and you can verify as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0db886ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-1.) tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "flat=first_song.reshape(-1,)\n",
    "print(min(flat), max(flat))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc873ff",
   "metadata": {},
   "source": [
    "If you recall, in image GANs, we also normalize all image data (i.e., pixels) to the range -1 to 1 and gradually adjust the values during training. The MuseGAN takes a similar approach, with the exception that the numbers represent music notes instead of image pixels. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e0b2e3",
   "metadata": {},
   "source": [
    "## 1.2. Convert Data to Songs\n",
    "Right now, the songs are formatted as PyTorch tensors and ready to be fed to a neural network. But before we do that, we want to convert the songs to music formats and hear them so we know what type of songs the model will likely generate. \n",
    "\n",
    "Below we convert a song to a midi file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4823d8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'files/ch15/song1.mid'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.midi_util import convert_to_midi\n",
    "\n",
    "music_data=convert_to_midi(first_song.unsqueeze(0))\n",
    "music_data.write(\"midi\",\"files/ch15/song1.mid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55202435",
   "metadata": {},
   "source": [
    "Now go to the folder /files/ch15/ and open the file *song1.mid* with a music player and you should hear a short piece of piano music. It lasts a few seconds. Alternatively, you can run the following code cell and use the *music21* library to play it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c56ff65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                <div id=\"midiPlayerDiv2405\"></div>\n",
       "                <link rel=\"stylesheet\" href=\"https://cuthbertLab.github.io/music21j/css/m21.css\">\n",
       "                \n",
       "                <script>\n",
       "                require.config({\n",
       "                    paths: {\n",
       "                        'music21': 'https://cuthbertLab.github.io/music21j/releases/music21.debug',\n",
       "                    }\n",
       "                });\n",
       "                require(['music21'], function(music21) {\n",
       "                    mp = new music21.miditools.MidiPlayer();\n",
       "                    mp.addPlayer(\"#midiPlayerDiv2405\");\n",
       "                    mp.base64Load(\"data:audio/midi;base64,TVRoZAAAAAYAAQAFBABNVHJrAAAAFAD/UQMN3yMA/1gEBAIYCIgA/y8ATVRyawAAAQ0A/wMAAOAAQADAAIgAkExahACATAAAkEdaggCARwAAkEhaggCASAAAkEpaggCASgAAkEhaggCASAAAkEdaggCARwAAkEVaggCARQAAkEVaggCARQAAkEhaggCASAAAkEdaggCARwAAkEVaggCARQAAkEVahgCARQAAkEBaggCAQAAAkDxaggCAPAAAkDlaggCAOQAAkEBahACAQAAAkD5aggCAPgAAkDxaggCAPAAAkEBaggCAQAAAkDxaggCAPAAAkC1aggCALQAAkDlaggCAOQAAkDhaggCAOAAAkDZaggCANgAAkDZaggCANgAAkDlaggCAOQAAkDRaggCANAAAkC1aggCALQCIAP8vAE1UcmsAAAENAP8DAADgAEAAwACIAJBMWoQAgEwAAJBHWoIAgEcAAJBIWoIAgEgAAJBKWoIAgEoAAJBIWoIAgEgAAJBHWoIAgEcAAJBFWoIAgEUAAJBFWoIAgEUAAJBIWoIAgEgAAJBHWoIAgEcAAJBFWoIAgEUAAJBFWoYAgEUAAJBAWoIAgEAAAJA8WoIAgDwAAJA5WoIAgDkAAJBAWoQAgEAAAJA+WoIAgD4AAJA8WoIAgDwAAJBAWoIAgEAAAJA8WoIAgDwAAJAtWoIAgC0AAJA5WoIAgDkAAJA4WoIAgDgAAJA2WoIAgDYAAJA2WoIAgDYAAJA5WoIAgDkAAJA0WoIAgDQAAJAtWoIAgC0AiAD/LwBNVHJrAAABFgD/AwAA4ABAAMAAiACQTFqEAIBMAACQR1qCAIBHAACQSFqCAIBIAACQSlqCAIBKAACQR1qEAIBHAACQRVqCAIBFAACQR1qCAIBHAACQSFqCAIBIAACQR1qCAIBHAACQRFqCAIBEAACQR1qCAIBHAACQRVqCAIBFAACQRFqCAIBEAACQQFqCAIBAAACQO1qCAIA7AACQOVqCAIA5AACQQFqEAIBAAACQO1qCAIA7AACQPlqCAIA+AACQQFqCAIBAAACQPFqCAIA8AACQLVqCAIAtAACQOVqCAIA5AACQOFqCAIA4AACQNFqCAIA0AACQOFqCAIA4AACQOVqCAIA5AACQNFqCAIA0AACQLVqCAIAtAIgA/y8ATVRyawAAARYA/wMAAOAAQADAAIgAkExahACATAAAkEdaggCARwAAkEhaggCASAAAkEpaggCASgAAkEdahACARwAAkEVaggCARQAAkEdaggCARwAAkEhaggCASAAAkEdaggCARwAAkERaggCARAAAkEdaggCARwAAkEVaggCARQAAkERaggCARAAAkEBaggCAQAAAkDtaggCAOwAAkDlaggCAOQAAkEBahACAQAAAkDtaggCAOwAAkD5aggCAPgAAkEBaggCAQAAAkDxaggCAPAAAkC1aggCALQAAkDlaggCAOQAAkDhaggCAOAAAkDRaggCANAAAkDhaggCAOAAAkDlaggCAOQAAkDRaggCANAAAkC1aggCALQCIAP8vAA==\");\n",
       "                });\n",
       "                </script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from music21 import midi\n",
    "\n",
    "mf = midi.MidiFile()\n",
    "mf.open(\"files/ch15/song1.mid\") \n",
    "mf.read()\n",
    "mf.close()\n",
    "stream = midi.translate.midiFileToStream(mf)\n",
    "stream.show('midi')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f1f903",
   "metadata": {},
   "source": [
    "Below, we extract the second and third songs and convert them into one single piece of longer music, lasting about 16 seconds: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08062972",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'files/ch15/song2.mid'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_songs=songs[1:3]\n",
    "music_data=convert_to_midi(two_songs)\n",
    "music_data.write(\"midi\",\"files/ch15/song2.mid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba706c3",
   "metadata": {},
   "source": [
    "Open the file *song2.mid* with a music player and you should hear a longer piece of piano music. Alternatively, you can press the play button below:\n",
    "\n",
    "<audio src=\"https://gattonweb.uky.edu/faculty/lium/ml/song2.mp3\" type=\"audio/mpeg\" controls=\"\" controlsList=\"nodownload\"></audio>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02df292",
   "metadata": {},
   "source": [
    "# 2.  Create A MuseGAN\n",
    "In this section, we create a deep 3-D convolutional GAN model so that we can train the model later to generate music pieces. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73036c27",
   "metadata": {},
   "source": [
    "## 2.1. A Critic in MuseGAN\n",
    "As we discussed in Chapter 7, using the Wasserstein distance in the loss function can stablize training. We therefore follow what we did in Chapter 7 and use a critic rather than a discriminator in the MuseGAN. Specifically, the critic is not a binary classifier. Instead, the critic evaluates the work by the generator (in this case, a piece of music) and returns a score between $-\\infty$ and $\\infty$. The higher the score, the better the quality of the music. \n",
    "\n",
    "We create a music critic neural network as follows, and it's defined in the file *MuseGAN_util.py* you just downloacded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc2f8827",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class MuseCritic(nn.Module):\n",
    "    def __init__(self,hid_channels=128,hid_features=1024,\n",
    "        out_features=1,n_tracks=4,n_bars=2,n_steps_per_bar=16,\n",
    "        n_pitches=84):\n",
    "        super().__init__()\n",
    "        self.n_tracks = n_tracks\n",
    "        self.n_bars = n_bars\n",
    "        self.n_steps_per_bar = n_steps_per_bar\n",
    "        self.n_pitches = n_pitches\n",
    "        in_features = 4 * hid_channels if n_bars == 2\\\n",
    "            else 12 * hid_channels\n",
    "        self.seq = nn.Sequential(\n",
    "            nn.Conv3d(self.n_tracks, hid_channels, \n",
    "                      (2, 1, 1), (1, 1, 1), padding=0),\n",
    "            nn.LeakyReLU(0.3, inplace=True),\n",
    "            nn.Conv3d(hid_channels, hid_channels, \n",
    "              (self.n_bars - 1, 1, 1), (1, 1, 1), padding=0),\n",
    "            nn.LeakyReLU(0.3, inplace=True),\n",
    "            nn.Conv3d(hid_channels, hid_channels, \n",
    "                      (1, 1, 12), (1, 1, 12), padding=0),\n",
    "            nn.LeakyReLU(0.3, inplace=True),\n",
    "            nn.Conv3d(hid_channels, hid_channels, \n",
    "                      (1, 1, 7), (1, 1, 7), padding=0),\n",
    "            nn.LeakyReLU(0.3, inplace=True),\n",
    "            nn.Conv3d(hid_channels, hid_channels, \n",
    "                      (1, 2, 1), (1, 2, 1), padding=0),\n",
    "            nn.LeakyReLU(0.3, inplace=True),\n",
    "            nn.Conv3d(hid_channels, hid_channels, \n",
    "                      (1, 2, 1), (1, 2, 1), padding=0),\n",
    "            nn.LeakyReLU(0.3, inplace=True),\n",
    "            nn.Conv3d(hid_channels, 2 * hid_channels, \n",
    "                      (1, 4, 1), (1, 2, 1), padding=(0, 1, 0)),\n",
    "            nn.LeakyReLU(0.3, inplace=True),\n",
    "            nn.Conv3d(2 * hid_channels, 4 * hid_channels, \n",
    "                      (1, 3, 1), (1, 2, 1), padding=(0, 1, 0)),\n",
    "            nn.LeakyReLU(0.3, inplace=True),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features, hid_features),\n",
    "            nn.LeakyReLU(0.3, inplace=True),\n",
    "            nn.Linear(hid_features, out_features))\n",
    "    def forward(self, x):  \n",
    "        return self.seq(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25216c46",
   "metadata": {},
   "source": [
    "The input to the critic model is a piece of music with a shape of 4 by 2 by 16 by 84. The Conv3d layer takes the music in each track as a 3-dimensinal object and applies filters on them to extract spatial features. The Conv3d layer works similarly to the conv2d layer on images except that the target is three-dimensional instead of two-dimensional. \n",
    "\n",
    "Notice that the last layer in the critic modle is linear and we don't apply an activation function on the ouput. Therefore, the output from the critic model is a value from $-\\infty$ to $\\infty$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1045f0",
   "metadata": {},
   "source": [
    "## 3.2. The Generator G in MuseGAN\n",
    "The generator G's job is to create a piece of music so that it can be rated as high as possible by the music critic. We create the following neural network to respresent the generator G:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3fa3af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MuseGenerator(nn.Module):\n",
    "    def __init__(self,z_dimension=32,hid_channels=1024,\n",
    "        hid_features=1024,out_channels=1,n_tracks=4,\n",
    "        n_bars=2,n_steps_per_bar=16,n_pitches=84):\n",
    "        super().__init__()\n",
    "        self.n_tracks = n_tracks\n",
    "        self.n_bars = n_bars\n",
    "        self.n_steps_per_bar = n_steps_per_bar\n",
    "        self.n_pitches = n_pitches\n",
    "        self.chords_network=TemporalNetwork(z_dimension, \n",
    "                            hid_channels, n_bars=n_bars)\n",
    "        self.melody_networks = nn.ModuleDict({})\n",
    "        for n in range(self.n_tracks):\n",
    "            self.melody_networks.add_module(\n",
    "                \"melodygen_\" + str(n),\n",
    "                TemporalNetwork(z_dimension, \n",
    "                 hid_channels, n_bars=n_bars))\n",
    "        self.bar_generators = nn.ModuleDict({})\n",
    "        for n in range(self.n_tracks):\n",
    "            self.bar_generators.add_module(\n",
    "                \"bargen_\" + str(n),BarGenerator(z_dimension,\n",
    "            hid_features,hid_channels // 2,out_channels,\n",
    "            n_steps_per_bar=n_steps_per_bar,n_pitches=n_pitches))\n",
    "    def forward(self,chords,style,melody,groove):\n",
    "        chord_outs = self.chords_network(chords)\n",
    "        bar_outs = []\n",
    "        for bar in range(self.n_bars):\n",
    "            track_outs = []\n",
    "            chord_out = chord_outs[:, :, bar]\n",
    "            style_out = style\n",
    "            for track in range(self.n_tracks):\n",
    "                melody_in = melody[:, track, :]\n",
    "                melody_out = self.melody_networks[\"melodygen_\"\\\n",
    "                          + str(track)](melody_in)[:, :, bar]\n",
    "                groove_out = groove[:, track, :]\n",
    "                z = torch.cat([chord_out, style_out, melody_out,\\\n",
    "                               groove_out], dim=1)\n",
    "                track_outs.append(self.bar_generators[\"bargen_\"\\\n",
    "                                          + str(track)](z))\n",
    "            track_out = torch.cat(track_outs, dim=1)\n",
    "            bar_outs.append(track_out)\n",
    "        out = torch.cat(bar_outs, dim=2)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea69e171",
   "metadata": {},
   "source": [
    "We'll feed random data from four difference latent spaces, each representing a differnet track, to the generator. The generator then generates a piece of music in the shape of (4, 2, 16, 84) with values between -1 and 1 based on inputs from the four latent spaces. \n",
    "\n",
    "Note that the *MuseGenerator()* class uses several other classes such as *BarGenerator()* and *TemporalNetwork()* that are defined in the file *MuseGAN_util.py*. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12dc77f7",
   "metadata": {},
   "source": [
    "## 3.3. Optimizers and the Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad7be94",
   "metadata": {},
   "source": [
    "We'll create a generator and a critic based on the *MuseGenerator()* and *MuseCritic()* classes in the local module, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "78d56b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from utils.MuseGAN_util import (\n",
    "    init_weights, MuseGenerator, MuseCritic)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "generator = MuseGenerator(z_dimension=32, hid_channels=1024, \n",
    "              hid_features=1024, out_channels=1).to(device)\n",
    "critic = MuseCritic(hid_channels=128,\n",
    "                    hid_features=1024,\n",
    "                    out_features=1).to(device)\n",
    "generator = generator.apply(init_weights)\n",
    "critic = critic.apply(init_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b56e69",
   "metadata": {},
   "source": [
    "Since the critic produces a rating rather than a classification, the loss function is defined as the negative average of the product between the prediction and the target, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd03b384",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(pred,target):\n",
    "    return -torch.mean(pred*target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f9cf2e",
   "metadata": {},
   "source": [
    "During training, for the generator, we'll set the target as 1 so the objective of the generator is to produce music so that the rating (i.e, the variable *pred* in the above function) can be as high as possible. For the critic, we'll set target to 1 for real music and -1 for fake music in the loss function. That is, the critic's job is to assign a high rating to real music and a low rating to fake music. \n",
    "\n",
    "Similar to what we did in Chapter 7, we add the Wasserstein distance with gradient penalty to the critic's loss function to stablize training. The gradient penalty is defined in the file *MuseGAN_util.py*, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "20658a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientPenalty(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def forward(self, inputs, outputs):\n",
    "        grad = torch.autograd.grad(\n",
    "            inputs=inputs,\n",
    "            outputs=outputs,\n",
    "            grad_outputs=torch.ones_like(outputs),\n",
    "            create_graph=True,\n",
    "            retain_graph=True,\n",
    "        )[0]\n",
    "        grad_=torch.norm(grad.view(grad.size(0),-1),p=2,dim=1)\n",
    "        penalty = torch.mean((1. - grad_) ** 2)\n",
    "        return penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2bf232e",
   "metadata": {},
   "source": [
    "We'll use the Adam optimizer for both the critic and the generator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "62249b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "g_optimizer = torch.optim.Adam(generator.parameters(),\n",
    "                               lr=lr, betas=(0.5, 0.9))\n",
    "c_optimizer = torch.optim.Adam(critic.parameters(),\n",
    "                               lr=lr, betas=(0.5, 0.9))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e27a43",
   "metadata": {},
   "source": [
    "# 3. Train the MuseGAN\n",
    "Now that we have the training data the and two networks, we'll train the MuseGAN model. After that, we'll discard the critic network and use the generator network to create multi-track music that resembles pieces from the traning set. \n",
    "\n",
    "First we define a few hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "371dc3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.MuseGAN_util import loss_fn, GradientPenalty\n",
    "\n",
    "batch_size=64\n",
    "repeat=5\n",
    "display_step=10\n",
    "epochs=500\n",
    "alpha=torch.rand((batch_size,1,1,1,1)).requires_grad_().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56d0e1a",
   "metadata": {},
   "source": [
    "The following function *train_epoch()* trains the model for one epoch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "384ac533",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch():\n",
    "    e_gloss = 0\n",
    "    e_closs = 0\n",
    "    for real in loader:\n",
    "        real = real.to(device)\n",
    "        # Train Critic\n",
    "        for _ in range(repeat):\n",
    "            chords = torch.randn(batch_size, 32).to(device)\n",
    "            style = torch.randn(batch_size, 32).to(device)\n",
    "            melody = torch.randn(batch_size, 4, 32).to(device)\n",
    "            groove = torch.randn(batch_size, 4, 32).to(device)\n",
    "            c_optimizer.zero_grad()\n",
    "            with torch.no_grad():\n",
    "                fake = generator(chords, style, melody,\\\n",
    "                                 groove).detach()\n",
    "            realfake = alpha * real + (1 - alpha) * fake\n",
    "            fake_pred = critic(fake)\n",
    "            real_pred = critic(real)\n",
    "            realfake_pred = critic(realfake)\n",
    "            fake_loss =  loss_fn(fake_pred, \\\n",
    "                                 - torch.ones_like(fake_pred))\n",
    "            real_loss = loss_fn(real_pred,\\\n",
    "                                torch.ones_like(real_pred))\n",
    "            gp = GradientPenalty()\n",
    "            penalty = gp(realfake, realfake_pred)\n",
    "            closs = fake_loss + real_loss + 10 * penalty\n",
    "            closs.backward(retain_graph=True)\n",
    "            c_optimizer.step()\n",
    "            e_closs += closs.item() / (repeat*len(loader))\n",
    "        # Train Generator\n",
    "        g_optimizer.zero_grad()\n",
    "        chords = torch.randn(batch_size, 32).to(device)\n",
    "        style = torch.randn(batch_size, 32).to(device)\n",
    "        melody = torch.randn(batch_size, 4, 32).to(device)\n",
    "        groove = torch.randn(batch_size, 4, 32).to(device)\n",
    "        fake = generator(chords, style, melody, groove)\n",
    "        fake_pred = critic(fake)\n",
    "        gloss = loss_fn(fake_pred, torch.ones_like(fake_pred))\n",
    "        gloss.backward()\n",
    "        g_optimizer.step()\n",
    "        e_gloss += gloss.item() / len(loader)\n",
    "    return e_gloss, e_closs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc02d1aa",
   "metadata": {},
   "source": [
    "The training process is very much like that we used in Chapter 7 when we train the conditional GAN with gradient penalty. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d561161b",
   "metadata": {},
   "source": [
    "We now train the model for 500 epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "04547ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1,501):\n",
    "    e_gloss, e_closs = train_epoch()\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, G loss {e_gloss} C loss {e_closs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d1a097",
   "metadata": {},
   "source": [
    "If you use GPU training, it takes about an hour. Otherwise, it may take several hours. Once done, you can save the model to the local folder as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e9d83cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(generator.state_dict(),'files/ch15/MuseGAN_G.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd8eed1",
   "metadata": {},
   "source": [
    "# 4. Music Generation with MuseGAN\n",
    "We first reload the generator as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1579eddb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator.load_state_dict(torch.load('files/ch15/MuseGAN_G.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f26be1",
   "metadata": {},
   "source": [
    "To generate music, we first sample from the latent spaces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bc02ddda",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pieces=5\n",
    "\n",
    "hords = torch.rand(num_pieces, 32)\n",
    "style = torch.rand(num_pieces, 32)\n",
    "melody = torch.rand(num_pieces, 4, 32)\n",
    "groove = torch.rand(num_pieces, 4, 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e30c501",
   "metadata": {},
   "source": [
    "Notice here I essentially generating five songs at once so we have a longer piece of music. You can change the value of the variable *num_pieces* to your own liking. \n",
    "\n",
    "We then feed the latent variables to the generator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e20401c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = generator(chords, style, melody, groove).detach()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acb9950",
   "metadata": {},
   "source": [
    "Finally, we convert the generated music to the midi format, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "70aaf5c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'files/ch15/MuseGAN_song.mid'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "music_data = convert_to_midi(preds.numpy())\n",
    "music_data.write('midi', 'files/ch15/MuseGAN_song.mid')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f71d8a2",
   "metadata": {},
   "source": [
    "You can listen to the gererated song like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2a1302bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                <div id=\"midiPlayerDiv23891\"></div>\n",
       "                <link rel=\"stylesheet\" href=\"https://cuthbertLab.github.io/music21j/css/m21.css\">\n",
       "                \n",
       "                <script>\n",
       "                require.config({\n",
       "                    paths: {\n",
       "                        'music21': 'https://cuthbertLab.github.io/music21j/releases/music21.debug',\n",
       "                    }\n",
       "                });\n",
       "                require(['music21'], function(music21) {\n",
       "                    mp = new music21.miditools.MidiPlayer();\n",
       "                    mp.addPlayer(\"#midiPlayerDiv23891\");\n",
       "                    mp.base64Load(\"data:audio/midi;base64,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\");\n",
       "                });\n",
       "                </script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mf = midi.MidiFile()\n",
    "mf.open(\"files/ch15/MuseGAN_song.mid\") \n",
    "mf.read()\n",
    "mf.close()\n",
    "stream = midi.translate.midiFileToStream(mf)\n",
    "stream.show('midi')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03a7cea",
   "metadata": {},
   "source": [
    "Or you can listen to the music by pressing the play button below:\n",
    "\n",
    "<audio src=\"https://gattonweb.uky.edu/faculty/lium/ml/MuseGAN_song.mp3\" type=\"audio/mpeg\" controls=\"\" controlsList=\"nodownload\"></audio>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

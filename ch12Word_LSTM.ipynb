{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "653089c9",
   "metadata": {},
   "source": [
    "# Chapter 12: Text Generation with Word Tokenization\n",
    "\n",
    "\n",
    "\n",
    "We'll discuss text generation with another approach: word tokenization. Instead of tokenizing characters, you’ll tokenize individual words (or a part of a word). However, you’ll not use the one-hot encoder to use a onehot variable (with value 1 in one place and 0 in all others) because we are dealing with tens of thousands of words, instead of less than 100 characters. Rather, you’ll use the embedding layer in PyTorch to efficiently encode the words. With that, you’ll train your LSTM to extract interconnection among different words out of vast amount of text data. This is a powerful tool in the real world since it allows you to automatically mine unstructured data such as social media posts, customer reviews, and analyst reports. \n",
    "\n",
    "When it comes to text generation with the trained model, the approach is similar to what we have done in Chapter 11 with character-level LSTM. We'll feed the prompt to the model to predict the next most likely word. You then add the prediction to the end of the prompt to form a new prompt. You repeat the process until the text reaches a certain length. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d1c19a",
   "metadata": {},
   "source": [
    "Start a new cell in ch12.ipynb and execute the following lines of code in it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "117477d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.makedirs(\"files/ch12\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c268aecc",
   "metadata": {},
   "source": [
    "# 1. Word-Level Tokenization\n",
    "We'll use the text file of Anna Karenina in one of Carlos Lara's GitHub repositories. Go to the link https://github.com/LeanManager/NLP-PyTorch/tree/master/data to download the text file and save it as *anna.txt* in the folder /Desktop/ai/files/ch12/ on your computer. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed84eadd",
   "metadata": {},
   "source": [
    "## 1.1. Clean Up the Text\n",
    "First, we load up the data and print out some passages to get a feeling about the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbfb8bd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Chapter', '1\\n\\n\\nHappy', 'families', 'are', 'all', 'alike;', 'every', 'unhappy', 'family', 'is', 'unhappy', 'in', 'its', 'own\\nway.\\n\\nEverything', 'was', 'in', 'confusion', 'in', 'the', \"Oblonskys'\", 'house.', 'The', 'wife', 'had\\ndiscovered', 'that', 'the', 'husband', 'was', 'carrying', 'on', 'an', 'intrigue', 'with', 'a', 'French\\ngirl,', 'who', 'had', 'been', 'a', 'governess', 'in', 'their', 'family,', 'and', 'she', 'had', 'announced', 'to\\nher', 'husband', 'that', 'she', 'could', 'not', 'go', 'on', 'living', 'in', 'the', 'same', 'house', 'with', 'him.\\nThis', 'position', 'of', 'affairs', 'had', 'now', 'lasted', 'three', 'days,', 'and', 'not', 'only', 'the\\nhusband', 'and', 'wife', 'themselves,', 'but', 'all', 'the', 'members', 'of', 'their', 'family', 'and\\nhousehold,', 'were', 'painfully', 'conscious', 'of', 'it.', 'Every', 'person', 'in', 'the', 'house\\nfelt', 'that', 'there', 'was', 'no', 'sense']\n"
     ]
    }
   ],
   "source": [
    "with open(\"files/ch12/anna.txt\",\"r\") as f:\n",
    "    text=f.read()\n",
    "words=text.split(\" \") \n",
    "print(words[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d073ca",
   "metadata": {},
   "source": [
    "The line break (\\n) is treated as part of the text, so we need to replace line breaks with white spaces. We also need to change all words to lower case so *The* and *the* are the same. Further, punctuations need to have a space in front of them so they are separated from words. For that purpose, we'll print out all unique character in the text as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c57af8c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'g', '*', '$', 'p', ')', 'e', '(', 'w', '4', '6', \"'\", 'o', ',', 'l', '\"', 'r', 'i', 'u', 'd', 'q', ':', 'n', '2', '9', '&', 'f', '.', '0', 'y', '/', '?', '3', ';', '8', 'x', '1', 'v', 'b', '_', '%', 's', 'j', ' ', 'z', 'm', 't', 'h', 'a', 'c', 'k', '!', '5', '7', '\\n', '-', '`', '@'}\n"
     ]
    }
   ],
   "source": [
    "print(set(text.lower()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7f5710",
   "metadata": {},
   "source": [
    "We can then go over each character and see if we need to do something about it. The following are the steps to clean up the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d463840",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text=text.lower().replace(\"\\n\", \" \")\n",
    "clean_text=clean_text.replace(\"-\", \" \")\n",
    "for x in \",.:;?!$()/_&%*@'`\":\n",
    "    clean_text=clean_text.replace(f\"{x}\", f\" {x} \")\n",
    "clean_text=clean_text.replace('\"', ' \" ') \n",
    "clean_text=clean_text.replace(\"     \", \" \")\n",
    "clean_text=clean_text.replace(\"    \", \" \")\n",
    "clean_text=clean_text.replace(\"   \", \" \")\n",
    "clean_text=clean_text.replace(\"  \", \" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce10d036",
   "metadata": {},
   "source": [
    "We can now save the cleaned up text as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0db886ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"files/ch12/cleaned_up.txt\",\"w\") as f:\n",
    "    f.write(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e0b2e3",
   "metadata": {},
   "source": [
    "## 1.2. Preprocess the Data\n",
    "We first create a PyTorch dataset based on the text file *cleaned_up.txt*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4823d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from collections import Counter\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class Data(Dataset):\n",
    "    def __init__(self,seq_len=50):\n",
    "        super().__init__()\n",
    "        self.text=self.get_text()\n",
    "        self.words=self.get_unique_words()\n",
    "        self.int_to_word={k:v for k,v in enumerate(self.words)}\n",
    "        self.word_to_int={v:k for k,v in enumerate(self.words)}        \n",
    "        self.wordidx=[self.word_to_int[w] for w in self.text]  \n",
    "        self.seq_len=seq_len\n",
    "    def get_text(self):\n",
    "        with open(\"files/ch12/cleaned_up.txt\",\"r\") as f:\n",
    "            text=f.read()\n",
    "        return text.split(\" \")\n",
    "    def get_unique_words(self):\n",
    "        word_counts = Counter(self.text)\n",
    "        return sorted(word_counts, key=word_counts.get,\n",
    "                      reverse=True)\n",
    "    def __len__(self):\n",
    "        return len(self.wordidx) - self.seq_len\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return (\n",
    "        torch.tensor(self.wordidx[i:i+self.seq_len]),\n",
    "        torch.tensor(self.wordidx[i+1:i+self.seq_len+1]),\n",
    "        )  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f1f903",
   "metadata": {},
   "source": [
    "We can now instantiate the dataset and see its properties:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08062972",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=Data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba706c3",
   "metadata": {},
   "source": [
    "We'll check the length of the training data and the number of unique words in it, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a6a600a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the text length is 440905 word\n",
      "there are 13000 num_unique_words\n"
     ]
    }
   ],
   "source": [
    "text_length=len(data.text)\n",
    "num_unique_words=len(data.words)\n",
    "print(f\"the text length is {text_length} word\")\n",
    "print(f\"there are {num_unique_words} num_unique_words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73036c27",
   "metadata": {},
   "source": [
    "We'll print out the first 20 words in the original text and then look at the individual words and their corresponding index numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc2f8827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['chapter', '1', 'happy', 'families', 'are', 'all', 'alike', ';', 'every', 'unhappy', 'family', 'is', 'unhappy', 'in', 'its', 'own', 'way', '.', 'everything', 'was']\n",
      "[208, 670, 283, 3024, 82, 31, 2461, 35, 202, 690, 365, 38, 690, 10, 234, 147, 166, 1, 149, 12]\n"
     ]
    }
   ],
   "source": [
    "print(data.text[0:20])\n",
    "print([data.word_to_int[w] for w in data.text[0:20]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02df292",
   "metadata": {},
   "source": [
    "## 1.3.  Create Batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b56e69",
   "metadata": {},
   "source": [
    "We'll organize the text into different batches so that we can feed them to the model to train the LSTM network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62249b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "loader = DataLoader(data, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba403f7",
   "metadata": {},
   "source": [
    "We print out a batch to have a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6943ca3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 208,  670,  283,  ...,   52,    9,  936],\n",
      "        [ 670,  283, 3024,  ...,    9,  936,   10],\n",
      "        [ 283, 3024,   82,  ...,  936,   10,   71],\n",
      "        ...,\n",
      "        [ 129,   18, 2187,  ...,  186,    6,  965],\n",
      "        [  18, 2187,   11,  ...,    6,  965,   18],\n",
      "        [2187,   11,    2,  ...,  965,   18,   58]])\n",
      "tensor([[ 670,  283, 3024,  ...,    9,  936,   10],\n",
      "        [ 283, 3024,   82,  ...,  936,   10,   71],\n",
      "        [3024,   82,   31,  ...,   10,   71,  365],\n",
      "        ...,\n",
      "        [  18, 2187,   11,  ...,    6,  965,   18],\n",
      "        [2187,   11,    2,  ...,  965,   18,   58],\n",
      "        [  11,    2,  159,  ...,   18,   58, 2188]])\n",
      "torch.Size([32, 50])\n"
     ]
    }
   ],
   "source": [
    "x,y=next(iter(loader))\n",
    "print(x)\n",
    "print(y)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a85aa66",
   "metadata": {},
   "source": [
    "The above results indicate that if you shift x one position to the right, you have y. That's exactly what we intend to do. We'll use x as features and y as targets. By using the above training data, the model learns to predict the next word based on the prompt. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e27a43",
   "metadata": {},
   "source": [
    "# 2. Build and Train the LSTM Model\n",
    "We'll use the built-in LSTM layer in PyTorch to create the model.\n",
    "\n",
    "## 2.1. The Model Structure\n",
    "We first import needed modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e8487434",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "device=\"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb3b029",
   "metadata": {},
   "source": [
    "We then define a *WordLSTM()* class to represent the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "730313c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordLSTM(nn.Module):\n",
    "    def __init__(self, input_size=128, n_embed=128,\n",
    "             n_layers=3, drop_prob=0.2):\n",
    "        super().__init__()\n",
    "        self.input_size=input_size\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_embed = n_embed\n",
    "        vocab_size=len(data.words)\n",
    "        self.embedding=nn.Embedding(vocab_size,n_embed)\n",
    "        self.lstm = nn.LSTM(input_size=self.input_size,\n",
    "            hidden_size=self.n_embed,\n",
    "            num_layers=self.n_layers,\n",
    "            dropout=self.drop_prob,batch_first=True)\n",
    "        self.fc = nn.Linear(input_size, vocab_size)\n",
    "\n",
    "    def forward(self, x, hc):\n",
    "        embed=self.embedding(x)\n",
    "        x, hc = self.lstm(embed, hc)\n",
    "        x = self.fc(x)\n",
    "        return x, hc      \n",
    "        \n",
    "    def init_hidden(self, n_seqs):\n",
    "        weight = next(self.parameters()).data\n",
    "        return (weight.new(self.n_layers,\n",
    "                           n_seqs, self.n_embed).zero_(),\n",
    "                weight.new(self.n_layers,\n",
    "                           n_seqs, self.n_embed).zero_()) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56d0e1a",
   "metadata": {},
   "source": [
    "## 2.2. Create the Model\n",
    "We first instantiate a model as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0d429629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WordLSTM(\n",
      "  (embedding): Embedding(13000, 128)\n",
      "  (lstm): LSTM(128, 128, num_layers=3, batch_first=True, dropout=0.2)\n",
      "  (fc): Linear(in_features=128, out_features=13000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model=WordLSTM().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc02d1aa",
   "metadata": {},
   "source": [
    "The optimizer and the loss function are as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8c746b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=0.0001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d561161b",
   "metadata": {},
   "source": [
    "We'll train the Model next."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd8eed1",
   "metadata": {},
   "source": [
    "# 3. Train the Model\n",
    "We first define some hyperparameter values and get ready for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7e26153e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordLSTM(\n",
       "  (embedding): Embedding(13000, 128)\n",
       "  (lstm): LSTM(128, 128, num_layers=3, batch_first=True, dropout=0.2)\n",
       "  (fc): Linear(in_features=128, out_features=13000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_seqs=32\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d94cc0",
   "metadata": {},
   "source": [
    "We then train the model for 20 epochs, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fe819235",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for epoch in range(10):\n",
    "    tloss=0\n",
    "    sh,sc = model.init_hidden(n_seqs)\n",
    "    for i, (x,y) in enumerate(loader):\n",
    "        if x.shape[0]==n_seqs:\n",
    "            inputs, targets = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output, (sh,sc) = model(inputs, (sh,sc))\n",
    "            loss = loss_func(output.transpose(1,2),targets)\n",
    "            sh,sc=sh.detach(),sc.detach()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
    "            optimizer.step()\n",
    "            tloss+=loss.item()\n",
    "        if (i+1)%1000==0:\n",
    "            print(f\"at epoch {epoch} iteration {i+1}\\\n",
    "            average loss = {tloss/(i+1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f7afb2",
   "metadata": {},
   "source": [
    "If you are using GPU, it takes an hour or so to train. If you use CPU only, it may take several hours, depending on your hardware. \n",
    "\n",
    "Next, we save the model on the local computer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d9effa9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),\"files/ch12/wordLSTM.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441286c7",
   "metadata": {},
   "source": [
    "# 4. Use the Trained Modle to Generate Text\n",
    "We can use the trained model to generate text. We first define the following sample() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "252eb61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(model, prompt=\"Anna\", length=200):\n",
    "    model.eval()\n",
    "    text = prompt.lower().split(' ')\n",
    "    sh,sc = model.init_hidden(1)\n",
    "\n",
    "    for i in range(0, length):\n",
    "        x = torch.tensor([[data.word_to_int[w] for w in text[i:]]])\n",
    "        inputs = x.to(device)\n",
    "        output, (sh,sc) = model(inputs, (sh,sc))\n",
    "        logits = output[0][-1]\n",
    "        p = nn.functional.softmax(logits, dim=0).detach().cpu().numpy()\n",
    "        idx = np.random.choice(len(logits), p=p)\n",
    "        text.append(data.int_to_word[idx])\n",
    "\n",
    "    return \" \".join(text)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34f4811",
   "metadata": {},
   "source": [
    "In the *sample()* function, we give a prompt so the function know where to start. The default prompt is \"Anna\". You can also specify the length of the text you want to generate and the default length is 200 words. The function then uses the trained model to predict the next word based on the existing text. It then adds the predicted word to the text. The function repeats the process until the text reaches the desired length. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c148f7c9",
   "metadata": {},
   "source": [
    "We then reload the trained model as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4f972d36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"files/ch12/wordLSTM.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bcaae3",
   "metadata": {},
   "source": [
    "Let's generate a passage with the model by using \"Anna and the\" as the prompt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1bbd71d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anna and the doctor could cut through the forces of frank them made great special extracts in the affair . cord were particularly well listening to his brother ' s face . \" what wicked and old life with kitty , she used to say , with which she had met her husband , which was stronger . anna arkadyevna , to hear the bitterness of his farming . the peasants steeplechase , both , and herself , for having sacrificed her share . she was not so bored since she saw it , not in his any never thought of his brother . he used to without indifferent to the laborer , the same expression of the hall bright and effective \" he said , with his hat from his damp hands . the grass @ something . the remainder had merrily . 17 sports . general for the rest of the dinner on russia , or ceasing , with everyone that suggested who was not going to feel completely right . his feelings was difficult to get the end of the imperial moment while , in spite of the same mode of view of the sisters she clambered away down flat\n"
     ]
    }
   ],
   "source": [
    "print(sample(model, prompt='Anna and the'))  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9bc75d",
   "metadata": {},
   "source": [
    "Notice that everything is lower case since we converted all upper case letters to lower cases when processing the text to reduce the number of potential words. Also notice that there is a white space before and after the punctuations because we want to separate punctuations from words during training. \n",
    "\n",
    "The above generated text is not bad for an hour of training! Most of sentences are correct in terms of grammar. It's not as good as the text generated by, say, ChatGPT. But you learned how to create a language model based on word-level tokenization and use LSTM to train a model to generate text. We'll discuss how to train a transformer -- the type of models that's used by ChatGPT, in the next chapter. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

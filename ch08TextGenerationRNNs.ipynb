{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "653089c9",
   "metadata": {},
   "source": [
    "# Chapter 8: Text Generation with Recurrent Neural Networks (RNNs)\n",
    "\n",
    "\n",
    "This chapter covers\n",
    "\n",
    "* The idea behind RNNs and why they can handle sequential data \n",
    "* Character tokenization, word tokenization, and subword tokenization\n",
    "* How word embedding works\n",
    "* Building and training an RNN to generate text \n",
    "* Using temperature and top-K sampling to control the creativeness of text generation\n",
    "\n",
    "So far in this book, you have learned to generate shapes, numbers, and images. Starting from this chapter, we’ll focus mainly on text generation. Generating text is often considered the holy grail of generative AI for several compelling reasons. Human language is incredibly complex and nuanced. It involves not only understanding grammar and vocabulary but also context, tone, and cultural references. Successfully generating coherent and contextually appropriate text is a significant challenge that requires deep understanding and processing of language. \n",
    "\n",
    "As humans, we primarily communicate through language. AI that can generate human-like text can interact more naturally with users, making technology more accessible and user-friendly. Text generation has many applications, from automating customer service responses to creating entire articles, scripting for games and movies, aiding in creative writing, and even building personal assistants. The potential impact across industries is enormous.\n",
    "\n",
    "In this chapter, you’ll make your first attempt at building and training models to generate text. You’ll learn to tackle three main challenges in modeling text generation. Firstly, text is sequential data, meaning the arrangement of words in a sentence is crucial. Predicting outcomes for sequences is challenging due to their sensitive ordering. Altering the sequence of elements changes their meaning. Secondly, the meaning of a certain part of the text depends on elements that appeared much earlier in the text. Understanding and modeling these long-range dependencies is essential for generating coherent text. Lastly, Human language is ambiguous and context-dependent. Training a model to understand nuances, sarcasm, idioms, and cultural references to generate contextually accurate text is challenging.\n",
    "\n",
    "You'll explore a specific neural network adept at handling sequential data, such as text or time series: the recurrent neural network (RNN). Unlike typical neural networks with one-directional neuron connections—from the input layer through hidden layers to the output layer—RNNs feature bi-directional connections, allowing feedback from later to earlier layers. We’ll focus on one variant of RNN, Long Short-Term Memory (LSTM) networks, which can recognize both short-term and long-term data patterns in sequential data like text. LSTM models use a hidden state to capture information in previous time steps. Therefore, a trained LSTM model can produce coherent text based on the context. \n",
    "\n",
    "The style of the generated text depends on the training data. You’ll use the text from the novel Anna Karenina to train an LSTM model. Since neural networks like an LSTM cannot accept text as input directly, you’ll learn to break down text into tokens (individual words or parts of words), a process known as tokenization. You’ll then create a dictionary to map each unique token into an integer (i.e., an index). Based on this dictionary, you’ll convert the text into a long sequence of integers, ready to be fed into a neural network. \n",
    "\n",
    "You’ll use sequences of indexes of a certain length as the input to train the LSTM model. You shift the sequence of inputs by one token to the right and use it as the output: you are effectively training the model to predict the next word in a sentence. A simplified example is as follows: the indexes corresponding to the four words “a frog has four” are used as input and those corresponding to “frog has four legs” as output. The model learns to use the word “a” to predict “frog”, to use “a frog” to predict “has”, and so on. Finally, the model uses “a frog has four” to predict the word “legs.” This is the so-called sequence-to-sequence prediction problem in natural language processing (NLP) and you’ll see it again in later chapters. \n",
    "\n",
    "Once the LSTM is trained, you’ll use it to generate text. You feed a prompt (part of a sentence such as “Anna and the”) to the trained model. The model predicts what’s the most likely next token and appends the selected token to your prompt. The new prompt is used again as the input; we feed this new prompt to the model again to predict the next token. The process continues until the prompt reaches a certain length. You’ll witness the trained LSTM model generating grammatically correct and coherent text, with a style matching that of the original novel. \n",
    "\n",
    "Finally, you also learn how to control the creativeness of the generated text by using temperature and top-K sampling. Temperature controls the randomness of the predictions of the trained model by scaling the logits (the inputs to the softmax function for probability calculation). A high temperature makes the generated text more creative while a low temperature makes the text more confident and predictable. Top-K sampling is a method where you select the K words with the highest probabilities instead of all words from the vocabulary. A small value of K leads to the selection of highly likely words in each step and this, in turn, makes the generated text less creative and more coherent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d1c19a",
   "metadata": {},
   "source": [
    "# 1.    Introduction to recurrent neural networks (RNNs)\n",
    "# 2.\tFundamentals of Natural Language Processing (NLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c268aecc",
   "metadata": {},
   "source": [
    "# 3.\tPrepare data to train the LSTM model\n",
    "We'll use the text file of Anna Karenina in one of Carlos Lara's GitHub repositories. Go to the link https://github.com/LeanManager/NLP-PyTorch/tree/master/data to download the text file and save it as *anna.txt* in the folder /files/ on your computer. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed84eadd",
   "metadata": {},
   "source": [
    "## 3.1\tDownload the clean up the text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbfb8bd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Chapter', '1\\n\\n\\nHappy', 'families', 'are', 'all', 'alike;', 'every', 'unhappy', 'family', 'is', 'unhappy', 'in', 'its', 'own\\nway.\\n\\nEverything', 'was', 'in', 'confusion', 'in', 'the', \"Oblonskys'\"]\n"
     ]
    }
   ],
   "source": [
    "with open(\"files/anna.txt\",\"r\") as f:\n",
    "    text=f.read()\n",
    "words=text.split(\" \") \n",
    "print(words[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c57af8c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'r', 'u', ';', \"'\", '!', '.', '*', 'n', '@', 'f', 'q', ':', '(', '3', '7', '8', '9', 'p', 'x', '6', 't', 'e', 'i', 'v', '4', '&', 'c', 'o', '\\n', 'z', ',', 's', '?', '2', '`', ' ', 'k', '0', 'l', ')', '_', '-', 'd', 'w', 'm', '5', 'a', 'j', '1', '/', 'y', '$', 'g', '\"', 'h', '%', 'b'}\n"
     ]
    }
   ],
   "source": [
    "print(set(text.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d463840",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text=text.lower().replace(\"\\n\", \" \")\n",
    "clean_text=clean_text.replace(\"-\", \" \")\n",
    "for x in \",.:;?!$()/_&%*@'`\":\n",
    "    clean_text=clean_text.replace(f\"{x}\", f\" {x} \")\n",
    "clean_text=clean_text.replace('\"', ' \" ') \n",
    "text=clean_text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47d983a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[',', '.', 'the', '\"', 'and', 'to', 'of', 'he', \"'\", 'a']\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter   \n",
    "word_counts = Counter(text)    \n",
    "\n",
    "# get unique words\n",
    "words=sorted(word_counts, key=word_counts.get,\n",
    "                      reverse=True) \n",
    "print(words[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ecca0e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the text contains 440902 words\n",
      "there are 12999 unique tokens\n",
      "{',': 0, '.': 1, 'the': 2, '\"': 3, 'and': 4, 'to': 5, 'of': 6, 'he': 7, \"'\": 8, 'a': 9}\n",
      "{0: ',', 1: '.', 2: 'the', 3: '\"', 4: 'and', 5: 'to', 6: 'of', 7: 'he', 8: \"'\", 9: 'a'}\n"
     ]
    }
   ],
   "source": [
    "text_length=len(text)\n",
    "num_unique_words=len(words)\n",
    "print(f\"the text contains {text_length} words\")\n",
    "print(f\"there are {num_unique_words} unique tokens\")  \n",
    "word_to_int={v:k for k,v in enumerate(words)} \n",
    "int_to_word={k:v for k,v in enumerate(words)}\n",
    "print({k:v for k,v in word_to_int.items() if k in words[:10]})\n",
    "print({k:v for k,v in int_to_word.items() if v in words[:10]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1f21f00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['chapter', '1', 'happy', 'families', 'are', 'all', 'alike', ';', 'every', 'unhappy', 'family', 'is', 'unhappy', 'in', 'its', 'own', 'way', '.', 'everything', 'was']\n",
      "[208, 670, 283, 3024, 82, 31, 2461, 35, 202, 690, 365, 38, 690, 10, 234, 147, 166, 1, 149, 12]\n"
     ]
    }
   ],
   "source": [
    "print(text[0:20])\n",
    "wordidx=[word_to_int[w] for w in text]  \n",
    "print([word_to_int[w] for w in text[0:20]])  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e0b2e3",
   "metadata": {},
   "source": [
    "## 3.2\tCreate batches of training data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4823d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "seq_len=50  \n",
    "xys=[]\n",
    "for n in range(0, len(wordidx)-seq_len-1):\n",
    "    x = wordidx[n:n+seq_len]\n",
    "    y = wordidx[n+1:n+seq_len+1]\n",
    "    xys.append((torch.tensor(x),(torch.tensor(y))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08062972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    6,    21,     5,  ..., 11718,     0,     4],\n",
      "        [   35,   116,    32,  ...,   116,     0,    21],\n",
      "        [    2,   431,    38,  ...,     1,    37,    21],\n",
      "        ...,\n",
      "        [   32,  1760,    10,  ...,    45,    55,    20],\n",
      "        [ 1441,   317,     5,  ...,    89,   158,   111],\n",
      "        [   57,    33,   585,  ...,    14,  1020,  1244]])\n",
      "tensor([[  21,    5,   43,  ...,    0,    4, 1477],\n",
      "        [ 116,   32, 3160,  ...,    0,   21,    8],\n",
      "        [ 431,   38,    5,  ...,   37,   21,    8],\n",
      "        ...,\n",
      "        [1760,   10,  128,  ...,   55,   20,   32],\n",
      "        [ 317,    5,  177,  ...,  158,  111,    0],\n",
      "        [  33,  585,    7,  ..., 1020, 1244,    0]])\n",
      "torch.Size([32, 50]) torch.Size([32, 50])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "torch.manual_seed(42)\n",
    "batch_size=32\n",
    "loader = DataLoader(xys, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "x,y=next(iter(loader))\n",
    "print(x)\n",
    "print(y)\n",
    "print(x.shape,y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e27a43",
   "metadata": {},
   "source": [
    "# 4. Build and Train the LSTM Model\n",
    "\n",
    "\n",
    "## 4.1\tBuild an LSTM model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "730313c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "class WordLSTM(nn.Module):\n",
    "    def __init__(self, input_size=128, n_embed=128,\n",
    "             n_layers=3, drop_prob=0.2):\n",
    "        super().__init__()\n",
    "        self.input_size=input_size\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_embed = n_embed\n",
    "        vocab_size=len(word_to_int)\n",
    "        self.embedding=nn.Embedding(vocab_size,n_embed)\n",
    "        self.lstm = nn.LSTM(input_size=self.input_size,\n",
    "            hidden_size=self.n_embed,\n",
    "            num_layers=self.n_layers,\n",
    "            dropout=self.drop_prob,batch_first=True)\n",
    "        self.fc = nn.Linear(input_size, vocab_size)\n",
    "    def forward(self, x, hc):\n",
    "        embed=self.embedding(x)\n",
    "        x, hc = self.lstm(embed, hc)\n",
    "        x = self.fc(x)\n",
    "        return x, hc        \n",
    "    def init_hidden(self, n_seqs):\n",
    "        weight = next(self.parameters()).data\n",
    "        return (weight.new(self.n_layers,\n",
    "                           n_seqs, self.n_embed).zero_(),\n",
    "                weight.new(self.n_layers,\n",
    "                           n_seqs, self.n_embed).zero_()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d429629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WordLSTM(\n",
      "  (embedding): Embedding(12999, 128)\n",
      "  (lstm): LSTM(128, 128, num_layers=3, batch_first=True, dropout=0.2)\n",
      "  (fc): Linear(in_features=128, out_features=12999, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model=WordLSTM().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c746b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=0.0001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd8eed1",
   "metadata": {},
   "source": [
    "## 4.2\tTrain the LSTM model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe819235",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.train()\n",
    "\n",
    "for epoch in range(10):\n",
    "    tloss=0\n",
    "    sh,sc = model.init_hidden(batch_size)\n",
    "    for i, (x,y) in enumerate(loader):    \n",
    "        if x.shape[0]==batch_size:\n",
    "            inputs, targets = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output, (sh,sc) = model(inputs, (sh,sc))\n",
    "            loss = loss_func(output.transpose(1,2),targets)\n",
    "            sh,sc=sh.detach(),sc.detach()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
    "            optimizer.step()\n",
    "            tloss+=loss.item()\n",
    "        if (i+1)%1000==0:\n",
    "            print(f\"at epoch {epoch} iteration {i+1}\\\n",
    "            average loss = {tloss/(i+1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d9effa9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "torch.save(model.state_dict(),\"files/wordLSTM.pth\")\n",
    "with open(\"files/word_to_int.p\",\"wb\") as fb:    \n",
    "    pickle.dump(word_to_int, fb) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441286c7",
   "metadata": {},
   "source": [
    "# 5\tGenerate text with the trained LSTM model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee861399",
   "metadata": {},
   "source": [
    "## 5.1\tGenerate text by predicting the next token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4f972d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "model.load_state_dict(torch.load(\"files/wordLSTM.pth\"))\n",
    "with open(\"files/word_to_int.p\",\"rb\") as fb:    \n",
    "    word_to_int = pickle.load(fb)      \n",
    "int_to_word={v:k for k,v in word_to_int.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "252eb61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def sample(model, prompt=\"Anna\", length=200):\n",
    "    model.eval()\n",
    "    text = prompt.lower().split(' ')\n",
    "    hc = model.init_hidden(1)\n",
    "    length = length - len(text)\n",
    "    for i in range(0, length):\n",
    "        # if the text length is less than 50, use text to predict \n",
    "        if len(text)<=50:\n",
    "            x = torch.tensor([[word_to_int[w] for w in text]])\n",
    "        # otherwise use the last 50 tokens to predict\n",
    "        else:\n",
    "            x = torch.tensor([[word_to_int[w] for w in text[-50:]]])            \n",
    "        inputs = x.to(device)\n",
    "        output, hc = model(inputs, hc)\n",
    "        logits = output[0][-1]\n",
    "        p = nn.functional.softmax(logits, dim=0).detach().cpu().numpy()\n",
    "        idx = np.random.choice(len(logits), p=p)\n",
    "        text.append(int_to_word[idx])\n",
    "    return \" \".join(text)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1bbd71d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anna and the prince said simply from her to go . after dinner from the messroom of the house , he told him to show of his son . no one went out , but he was disinclined in uncertainty . \" if i was getting , than i am . \" \" i envy them dolly ; but those it ' s all as i didn ' t want to marry , haven ' t . they must need nothing , and that we don ' t like it of any helping as he called them . they are often too , and to stop it . even that not ever he was going . since alexey alexandrovitch under her own truly was due so as to do him and needed , that his brother and his caresses . it was all discussions still and thoughts . he was angry in , and dropped his hand to mihail , looking round . at the races they heard the meadows , and the volunteers ran and listened . levin was silent . till the end of want over the elections were konstantin anna , and unconsciously leaning down in the\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "print(sample(model, prompt='Anna and the prince'))  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b838002",
   "metadata": {},
   "source": [
    "## 5.2\tTemperature and top-K sampling in text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9d74ad57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, prompt=\"Anna and the\", top_k=None, \n",
    "             length=200, temperature=1):\n",
    "    model.eval()\n",
    "    text = prompt.lower().split(' ')\n",
    "    hc = model.init_hidden(1)\n",
    "    length = length - len(text)    \n",
    "    for i in range(0, length):\n",
    "        # if the text length is less than 50, use text to predict \n",
    "        if len(text)<=50:\n",
    "            x = torch.tensor([[word_to_int[w] for w in text]])\n",
    "        # otherwise use the last 50 tokens to predict\n",
    "        else:\n",
    "            x = torch.tensor([[word_to_int[w] for w in text[-50:]]])    \n",
    "        inputs = x.to(device)\n",
    "        output, hc = model(inputs, hc)\n",
    "        logits = output[0][-1]\n",
    "        # scale the logits with the temperature \n",
    "        logits = logits/temperature\n",
    "        p = nn.functional.softmax(logits, dim=0).detach().cpu()    \n",
    "        if top_k is None:\n",
    "            idx = np.random.choice(len(logits), p=p.numpy())\n",
    "        # top-K sampling\n",
    "        else:\n",
    "            ps, tops = p.topk(top_k)\n",
    "            ps=ps/ps.sum()\n",
    "            idx = np.random.choice(tops, p=ps.numpy())          \n",
    "        text.append(int_to_word[idx])\n",
    "    return \" \".join(text)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8a79a77f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anna and the prince had said to him . \" i am not very glad . \" \" i ' m not going to see you . \" \" oh , i ' m not going to say to her . \" i ' m not going , \" she said , looking at his eyes , \" and what is it ? \" \" i am very glad to tell you that , \" she said . \" yes , i ' m very glad . \" \" i don ' t know , \" he said . \" i ' m not going to see you , \" she said , and she went on , and with a smile , he began telling him that he was in his own position . \" i ' ll tell you what , and what ' s the matter ? \" \" i ' ve been going to your house , \" she said , smiling . \" i am very glad to do you know , \" she said , looking round . \" i can ' t go . \" \" i ' ve never been at the\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "print(generate(model, prompt='Anna and the prince',\n",
    "               top_k=5,\n",
    "               temperature=0.6)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "18686bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anna and the prince seemed totally exhausted from as that he advised too calm , seams thrust for this interest . nothing made there either his attention was moving sometimes her ears alone , living her he divined materials false everything that presented nothing to drive . waving it reaching him by interest . splendidly art thronged undressed then flower , more and talking with him several public special split ' forever . hart painted out , bedsores beyond seventeen circles , now he derived friendly satisfaction , now levin goes under allusions were away clear clearly than regular 3 in fact , and and appeared her books awkward ; but perfectly soon he still went smoothly giving everything bitter breaths and unequal itself word exasperated . \" said anna chapter insufferably , two minutes now . but intentionally time they showed this proposition sharply from difficulty to under the earth ; \" they invite him to drink extracts of burden absolutely better many because over things so . telling me i ' ll allow . well , speaking . where were jolly of your mother grow smoothly and task become late for all the privileges decorum ! i must\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "print(generate(model, prompt='Anna and the prince',\n",
    "               top_k=None,\n",
    "               temperature=1.5)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "541d7525",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

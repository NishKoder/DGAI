{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "653089c9",
   "metadata": {},
   "source": [
    "# Chapter 9: Text Generation with Character-Level LSTM\n",
    "\n",
    "\n",
    "\n",
    "Starting from this chapter, we'll discuss text generation. Text is a sequence, meaning the orderings of individual characters or words matter. Sequence outcome is one of the most difficult events to predict because of this reason. If you switch the positions of the elements in a sequence, you change the meaning. The individual letters in a character-based sequence are ordered in a particular way and cannot be changed. This is in contrast to non-sequential data such as cross-sectional data. Imagine you have 1000 loan applications from different individuals and want to predict whether the loan will be approved based on applicant characteristics such as income, gender and so on. You can switch the positions of the applications of, say, John Smith and Maria Garcia, and this won’t affect the prediction outcome.\n",
    "\n",
    "In this chapter, you’ll learn in detail a special type of neural network that can handle sequential data such as texts or time series data: recurrent neural networks (RNNs). In most neural networks, the connection of neurons goes in one direction. The network starts with the input layer, and it goes through a series of hidden layers, and finally it reaches the output layer. In contrast, in RNNs, the connection goes in both directions: neurons in later layers also give feedback to previous layers. \n",
    "\n",
    "One shortcoming of RNNs is that the memory is relatively short. That is, it can detect short patterns in a sequence, but once you have a long pattern, RNNs take a long time to train. You’ll use a special type of RNNs: long short-term memory (LSTM) to address this problem. As the name suggests, LSTM models can capture both short-term and long-term patterns in the data. \n",
    "\n",
    "You'll treat text as a sequence of characters. The LSTM model learns the statistical patterns in the training dataset. After training, you can ask the model to predict the next character based on the prompt. You then add the prediction to the end of the prompt to form the new prompt. You repeat the process until the text reaches a certain length. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d1c19a",
   "metadata": {},
   "source": [
    "Start a new cell in ch11.ipynb and execute the following lines of code in it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "117477d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.makedirs(\"files/ch11\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c268aecc",
   "metadata": {},
   "source": [
    "# 1. Character-Level Tokenization\n",
    "We'll use the complete works of William Shakespeare by Project Gutenberg. Go to the link https://ocw.mit.edu/ans7870/6/6.006/s08/lecturenotes/files/t8.shakespeare.txt to download the text file and save it as *shakespeare.txt* in the folder /Desktop/ai/files/ch11/ on your computer. Make sure you remove the top and the bottom paragraphs from the text file so the file contains only the works of Shakespeare, not the descriptions added by Project Gutenberg. \n",
    "\n",
    "The code in this chapter is adapted from two wonderful GitHub repositories: Andrej Karpathy's char-rnn https://github.com/karpathy/char-rnn and Carlos Lara' Character-Level LSTM in Pytorch https://github.com/LeanManager/NLP-PyTorch. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e0b2e3",
   "metadata": {},
   "source": [
    "## 1.1. Preprocess the Data\n",
    "We first load the text file to Python and see how many individual characters the text contains:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4823d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84\n"
     ]
    }
   ],
   "source": [
    "with open(\"files/ch11/shakespeare.txt\",\"r\") as f:\n",
    "    file=f.read()\n",
    "characters=set(file)  \n",
    "vocab_size=len(characters)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f1f903",
   "metadata": {},
   "source": [
    "The output above shows that there are 84 different characters in the text, including punctuations. \n",
    "\n",
    "Next, we create two dictionaries: one maps a character to an integer and the other an integer to the character. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08062972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'D': 0, \"'\": 1, '-': 2, '_': 3, '>': 4, 'Y': 5, 'j': 6, 'O': 7, 'm': 8, '|': 9, '2': 10, '\\n': 11, 'L': 12, 'A': 13, 'g': 14, 'K': 15, 'M': 16, 'H': 17, '!': 18, 'Q': 19, 'V': 20, 'x': 21, 'B': 22, 'u': 23, 'F': 24, 'r': 25, '0': 26, '}': 27, 'a': 28, 'T': 29, 'w': 30, 'X': 31, 'y': 32, '<': 33, '8': 34, 'W': 35, ',': 36, '5': 37, 's': 38, 'U': 39, 't': 40, 'I': 41, ';': 42, '(': 43, 'f': 44, ']': 45, 'q': 46, 'p': 47, 'h': 48, '\"': 49, 'R': 50, 'c': 51, 'G': 52, 'e': 53, 'i': 54, 'S': 55, '6': 56, 'v': 57, '[': 58, 'Z': 59, '.': 60, 'J': 61, 'z': 62, '9': 63, 'E': 64, ':': 65, '4': 66, 'b': 67, 'k': 68, 'n': 69, 'C': 70, 'd': 71, '3': 72, '?': 73, '`': 74, '&': 75, 'o': 76, '1': 77, '7': 78, ')': 79, 'l': 80, 'N': 81, ' ': 82, 'P': 83}\n"
     ]
    }
   ],
   "source": [
    "char_to_int={}\n",
    "idx=0\n",
    "for char in characters:\n",
    "    char_to_int[char]=idx\n",
    "    idx+=1\n",
    "print(char_to_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba706c3",
   "metadata": {},
   "source": [
    "We'll switch the keys and values in the dictionary *char_to_int* and create a new dictionary *int_to_char*, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a6a600a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'D', 1: \"'\", 2: '-', 3: '_', 4: '>', 5: 'Y', 6: 'j', 7: 'O', 8: 'm', 9: '|', 10: '2', 11: '\\n', 12: 'L', 13: 'A', 14: 'g', 15: 'K', 16: 'M', 17: 'H', 18: '!', 19: 'Q', 20: 'V', 21: 'x', 22: 'B', 23: 'u', 24: 'F', 25: 'r', 26: '0', 27: '}', 28: 'a', 29: 'T', 30: 'w', 31: 'X', 32: 'y', 33: '<', 34: '8', 35: 'W', 36: ',', 37: '5', 38: 's', 39: 'U', 40: 't', 41: 'I', 42: ';', 43: '(', 44: 'f', 45: ']', 46: 'q', 47: 'p', 48: 'h', 49: '\"', 50: 'R', 51: 'c', 52: 'G', 53: 'e', 54: 'i', 55: 'S', 56: '6', 57: 'v', 58: '[', 59: 'Z', 60: '.', 61: 'J', 62: 'z', 63: '9', 64: 'E', 65: ':', 66: '4', 67: 'b', 68: 'k', 69: 'n', 70: 'C', 71: 'd', 72: '3', 73: '?', 74: '`', 75: '&', 76: 'o', 77: '1', 78: '7', 79: ')', 80: 'l', 81: 'N', 82: ' ', 83: 'P'}\n"
     ]
    }
   ],
   "source": [
    "int_to_char={v:k for k,v in char_to_int.items()}\n",
    "print(int_to_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73036c27",
   "metadata": {},
   "source": [
    "We'll encode the text to integer numbers so that we can further change them to one-hot variables later before feeding them to the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc2f8827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[77, 56, 26, 63, 11, 11, 29, 17, 64, 82, 55, 7, 81, 81, 64, 29, 55, 11, 11, 67]\n"
     ]
    }
   ],
   "source": [
    "encoded_text=[char_to_int[char] for char in file]\n",
    "print(encoded_text[0:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25216c46",
   "metadata": {},
   "source": [
    "Now the text are encoded as integers. The result above shows the first 20 elements in the encoded file. We'll also define a *onehot_encoder()* function to change an integer into a onehot variable with a depth equal to the number of characters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d01370ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def onehot_encode(arr, depth=vocab_size):\n",
    "    one_hot = np.zeros((np.multiply(*arr.shape), depth),\n",
    "                       dtype=np.float32)\n",
    "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
    "    one_hot = one_hot.reshape((*arr.shape, depth))\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02df292",
   "metadata": {},
   "source": [
    "## 1.2.  Create Batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b56e69",
   "metadata": {},
   "source": [
    "We'll organize the text into different batches so that we can feed them to the model to train the LSTM network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62249b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batch(arr, n_seqs, n_steps):\n",
    "    batch_size = n_seqs * n_steps\n",
    "    n_batches = len(arr)//batch_size\n",
    "    arr = arr[:n_batches * batch_size]\n",
    "    arr = arr.reshape((n_seqs, -1))\n",
    "    for n in range(0, arr.shape[1], n_steps):\n",
    "        x = arr[:, n:n+n_steps]\n",
    "        y = np.zeros_like(x)\n",
    "        try:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+n_steps]\n",
    "        except IndexError:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba403f7",
   "metadata": {},
   "source": [
    "To make sure that the function works as expected, you can set *n_seqs* to 5 and *n_steps* to 10. That is, each batch contains five sequences and each sequence has 10 characters: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6943ca3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[77 56 26 63 11 11 29 17 64 82]\n",
      " [31 29 82  7 24 82 41 12 12 41]\n",
      " [76 67 80 53 82 38 47 54 25 54]\n",
      " [53 28 57 53 82 48 54  8 11 82]\n",
      " [32 82 48 54 38 82 67 53 71 36]]\n",
      "[[56 26 63 11 11 29 17 64 82 55]\n",
      " [29 82  7 24 82 41 12 12 41 81]\n",
      " [67 80 53 82 38 47 54 25 54 40]\n",
      " [28 57 53 82 48 54  8 11 82 82]\n",
      " [82 48 54 38 82 67 53 71 36 11]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "batch=create_batch(np.array(encoded_text),5,10)\n",
    "x,y=next(batch)\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a85aa66",
   "metadata": {},
   "source": [
    "The above results indicate that if you shift x one position to the right, you have y. We'll use x as features and y as targets. By using training data like this, the model learns to predict the next character based on the prompt. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e27a43",
   "metadata": {},
   "source": [
    "# 2. Bulid and Train the LSTM Model\n",
    "We'll use the built-in LSTM layer in PyTorch to create the model.\n",
    "\n",
    "## 2.1. The Model Structure\n",
    "We first import needed modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8487434",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "device=\"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb3b029",
   "metadata": {},
   "source": [
    "We then define a *CharLSTM()* class to represent the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "730313c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharLSTM(nn.Module):\n",
    "    def __init__(self, vocab, n_steps=100, n_hidden=512,\n",
    "             n_layers=2, drop_prob=0.5, lr=0.001):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "        self.vocab = vocab\n",
    "        self.int2char = dict(enumerate(self.vocab))\n",
    "        self.char2int = {v:k for k,v in self.int2char.items()}\n",
    "        self.lstm = nn.LSTM(len(self.vocab), n_hidden, n_layers, \n",
    "                        dropout=drop_prob, batch_first=True)\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        self.fc = nn.Linear(n_hidden, len(self.vocab))\n",
    "        self.init_weights()\n",
    "        \n",
    "    def forward(self, x, hc):\n",
    "        x, (h, c) = self.lstm(x, hc)\n",
    "        x = self.dropout(x)\n",
    "        x = x.reshape(x.size()[0]*x.size()[1], self.n_hidden)\n",
    "        x = self.fc(x)\n",
    "        return x, (h, c)      \n",
    "\n",
    "    def predict(self, char, h=None, top_k=None):        \n",
    "        if h is None:\n",
    "            h = self.init_hidden(1)\n",
    "        x = np.array([[self.char2int[char]]])\n",
    "        x = onehot_encode(x, len(self.vocab))\n",
    "        inputs = torch.from_numpy(x).to(device)   \n",
    "        h = tuple([each.data for each in h])\n",
    "        out, h = self.forward(inputs, h)\n",
    "        p = F.softmax(out, dim=1).data\n",
    "        p = p.cpu()\n",
    "        if top_k is None:\n",
    "            top_ch = np.arange(len(self.vocab))\n",
    "        else:\n",
    "            p, top_ch = p.topk(top_k)\n",
    "            top_ch = top_ch.numpy().squeeze()\n",
    "        p = p.numpy().squeeze()\n",
    "        num = np.random.choice(top_ch, p=p/p.sum())  \n",
    "        return self.int2char[num], h\n",
    "    \n",
    "    def init_weights(self):\n",
    "        self.fc.bias.data.fill_(0)\n",
    "        self.fc.weight.data.uniform_(-1, 1)\n",
    "        \n",
    "    def init_hidden(self, n_seqs):\n",
    "        weight = next(self.parameters()).data\n",
    "        return (weight.new(self.n_layers,\n",
    "                           n_seqs, self.n_hidden).zero_(),\n",
    "                weight.new(self.n_layers,\n",
    "                           n_seqs, self.n_hidden).zero_())  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56d0e1a",
   "metadata": {},
   "source": [
    "## 2.2. Create the Model\n",
    "We first instantiate a model as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d429629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CharLSTM(\n",
      "  (lstm): LSTM(84, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc): Linear(in_features=512, out_features=84, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model=CharLSTM(characters).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc02d1aa",
   "metadata": {},
   "source": [
    "The optimizer and the loss function are as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c746b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d561161b",
   "metadata": {},
   "source": [
    "We'll train the Model next."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd8eed1",
   "metadata": {},
   "source": [
    "# 3. Train the Model\n",
    "We first define some hyperparameter values and get ready for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e26153e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CharLSTM(\n",
       "  (lstm): LSTM(84, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (fc): Linear(in_features=512, out_features=84, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_seqs=128\n",
    "n_steps=100\n",
    "data=np.array(encoded_text)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d94cc0",
   "metadata": {},
   "source": [
    "We then train the model for 30 epochs, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe819235",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at epoch 0 toal loss = 2.215922442604514\n",
      "at epoch 1 toal loss = 1.718651362026439\n",
      "at epoch 2 toal loss = 1.5548691931892844\n",
      "at epoch 3 toal loss = 1.4578687614553114\n",
      "at epoch 4 toal loss = 1.3951661878473618\n",
      "at epoch 5 toal loss = 1.3497756054822136\n",
      "at epoch 6 toal loss = 1.3160600796867818\n",
      "at epoch 7 toal loss = 1.2894528963986565\n",
      "at epoch 8 toal loss = 1.2677818831275491\n",
      "at epoch 9 toal loss = 1.2501716072419111\n",
      "at epoch 10 toal loss = 1.235118774806752\n",
      "at epoch 11 toal loss = 1.2218814403870526\n",
      "at epoch 12 toal loss = 1.21080896209268\n",
      "at epoch 13 toal loss = 1.200096674526439\n",
      "at epoch 14 toal loss = 1.1929191334107343\n",
      "at epoch 15 toal loss = 1.1838588605207556\n",
      "at epoch 16 toal loss = 1.1758920436746934\n",
      "at epoch 17 toal loss = 1.1689885487275966\n",
      "at epoch 18 toal loss = 1.1626772577622357\n",
      "at epoch 19 toal loss = 1.1572373996061438\n",
      "at epoch 20 toal loss = 1.151656112951391\n",
      "at epoch 21 toal loss = 1.1466644763946534\n",
      "at epoch 22 toal loss = 1.1412639940486236\n",
      "at epoch 23 toal loss = 1.1370210128671983\n",
      "at epoch 24 toal loss = 1.1329374189937815\n",
      "at epoch 25 toal loss = 1.1287004899978639\n",
      "at epoch 26 toal loss = 1.1250344789729398\n",
      "at epoch 27 toal loss = 1.1206640350117403\n",
      "at epoch 28 toal loss = 1.1176659511117375\n",
      "at epoch 29 toal loss = 1.114410145142499\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(30):\n",
    "    tloss=0\n",
    "    h = model.init_hidden(n_seqs)\n",
    "    n = 0\n",
    "    for x, y in create_batch(data, n_seqs, n_steps):\n",
    "        x = onehot_encode(x, vocab_size)\n",
    "        inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        h = tuple([each.data for each in h])\n",
    "        model.zero_grad()\n",
    "        output, h = model(inputs, h)\n",
    "        loss = loss_func(output, \n",
    "         targets.view(n_seqs*n_steps).type(torch.cuda.LongTensor))\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "        optimizer.step()\n",
    "        tloss+=loss.item()\n",
    "        n+=1\n",
    "    print(f\"at epoch {epoch} toal loss = {tloss/n}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f7afb2",
   "metadata": {},
   "source": [
    "If you are using GPU, it takes 30 minutes or so to train. If you use CPU only, it may take an hour or so to train, depending on your hardware. \n",
    "\n",
    "Next, we save the model on the local computer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d9effa9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),\"files/ch11/LSTMchar.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441286c7",
   "metadata": {},
   "source": [
    "# 4. Use the Trained Model to Generate Text\n",
    "We can use the trained model to generate text. We first define the following sample() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "252eb61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(model, size, prompt='The', top_k=None):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    chars = [ch for ch in prompt]\n",
    "    h = model.init_hidden(1)\n",
    "    for ch in prompt:\n",
    "        char, h = model.predict(ch, h, top_k=top_k)\n",
    "    chars.append(char)\n",
    "    for j in range(size):\n",
    "        char, h = model.predict(chars[-1], h, top_k=top_k)\n",
    "        chars.append(char)\n",
    "    return ''.join(chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f31a11f",
   "metadata": {},
   "source": [
    "We then reload the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4f972d36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"files/ch11/LSTMchar.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f37545e",
   "metadata": {},
   "source": [
    "Finally, we call the *sample()* function and use *The man* as the prompt to generate text up to 1000 characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1bbd71d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The manners that his pains\n",
      "      Shall hear thee at your powers, which he is stay.\n",
      "\n",
      "            Enter CORIOLANUS, ALONSO, and, as to the Duke.\n",
      "                Alexandria. The COUNT'S palace\n",
      "\n",
      "Enter ARMADO, the COSTARD\n",
      "\n",
      "  CASSIO. Why, madam!\n",
      "  CLOTEN. Thou wert not for my fortune; thy son, sir.\n",
      "  SEBASTIAN. Why, what as you do think?\n",
      "    I am sent in the poor comfort and this friends,\n",
      "    And see his happiness is as he charge.\n",
      "    Who can say so to your lieutenant?\n",
      "  CORIOLANUS. He had to hold this to the pain of silence.\n",
      "  ANGELO. I shall be sorry, to the proud credit her war.\n",
      "ANTIPHOLUS OF EPHESUS. And, she is nothing.\n",
      "\n",
      "                        Enter CORIOLANUS\n",
      "\n",
      "    He is my ludge, I will say the King's death.\n",
      "  PROTEUS. What, mother, sir?\n",
      "  PAROLLES. I do deliver her.  \n",
      "  BARDOLPH. A gentleman all a milk to make my hist till herself and her her\n",
      "    will to me, my good lord, it is any steps with a most, a maid\n",
      "    and stop of man is a song and to th' encounter of Salisbory.\n",
      "  CADE. Is it best a\n"
     ]
    }
   ],
   "source": [
    "print(sample(model, 1000, prompt='The man', top_k=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9bc75d",
   "metadata": {},
   "source": [
    "The results are actually not bad. The model certainly has grasped the Shakespearean style!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

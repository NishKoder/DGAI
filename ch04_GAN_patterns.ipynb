{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "653089c9",
   "metadata": {},
   "source": [
    "# Chapter 4: Generate Patterns\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "***\n",
    "“...a GAN is a pair of adversarial deep learning neural networks. The first network, the forger network, tries to generate something that looks real... The other network, the detective network, ... determines if the forger’s output is real or fake.”\n",
    "\n",
    "\n",
    "-- Kai-Fu Lee, former Prsident of Google China\n",
    "***\n",
    "\n",
    "\n",
    "You learned the basic idea behind Generative Adversarial Networks (GANs) in the previous chapter. You then implemented your first GAN by generating pairs of numbers that form an inverted-U shape. We have used the inverted-U shape as an example, but you can easily modify the code and gererate other shapes such as sine, cosine, U-shape and so on. The GAN framework can be applied to many different settings and researchers have generated images, audio, music, video and more by using the framework. \n",
    "\n",
    "To make this point clear, in this chapter, you'll learn how to use GAN to generate a sequence of numbers with certain patterns. We'll try to generate multiples of five. But you can change the pattern to multiples of two, three, seven, or any number really.\n",
    "\n",
    "To implement the idea, the key is to convert numbers to a format that neural networks understand. Let's say that we want to generate a sequence of ten numbers, and each is a multiple of five in the range 0 to 99. We cannot feed the ten numbers such as [15, 25, 0, 55...] to a neural network. Neural networks adjust values by small amounts in each iteration and we cannot teach the network to change values from one integer to another. Therefore, we will find a way to treat each of the 100 integer numbers, 0, 1, ..., 99, as a separate choice for the neural network. \n",
    "\n",
    "The answer lies in one-hot encoders. An integer between 0 and 99 can be represented by $quotient\\times5+remainder$, where the value of $quotient$ ranges from 0 to 19 and the value of $remainder$ ranges from 0 to 4. We'll use a 20-value one-hot variable to represent the quotient and a 4-value one-hot variable to represent the remainder. For example, the one-hot variable [1, 0, 0, 0, 0] indicates that the remainder is 0, while [0, 1, 0, 0, 0] indicates the remainder is 1, and so on.\n",
    "\n",
    "To create an integer between 0 and 99, the neural network need to pick a 20-value one-hot variable and a 4-value one-hot variable to represent the quotient and the remainder, respectively. For example, if the 20-value one-hot variable is\n",
    "$$[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]$$\n",
    "and the 4-value one-hot variable is \n",
    "$$[0, 1, 0, 0, 0],$$\n",
    "the neural network has select the integer $7\\times5+1=36$.\n",
    "\n",
    "If a number is a multiple of five, then the associated 4-value one-hot variable is always \n",
    "$[1, 0, 0, 0, 0].$ We'll create a GAN for this purpose and train the model with data. After the model is trained, we'll discard the discriminator and use the generator to generate sequence of numbers. The trained generator always generates multiples of five. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d1c19a",
   "metadata": {},
   "source": [
    "Start a new cell in ch04.ipynb and execute the following lines of code in it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "117477d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.makedirs(\"files/ch04\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282ad876",
   "metadata": {},
   "source": [
    "# 1. One-Hot Encoders\n",
    "In this chapter, you'll learn how to convert an integer number between 0 and 99 to two one-hot variables so that you can feed them to the neural networks. Similarly, you'll also convert two one-hot variables back to an integer between 0 and 99 so it's easy for human beings to understand. You basically translate between humans and machines. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7252a608",
   "metadata": {},
   "source": [
    "## 1.1. What Are One-Hot Variables?\n",
    "One-hot variables have value 1 in one place and 0 in all other places. In machine learning, we usually use one-hot encoding to represent *categorical variables*. Examples of categorical variables are the color of a house, which can be red, green, or white. We can use numbers 0, 1, and 2 to represent the three colors in some machine learning models. However, when dealing with neural networks, we need to convert categorical data to one-hot variables, so we use [1, 0, 0] to represent red, [0, 1, 0] to represent green, and  [0, 0, 1] white. \n",
    "\n",
    "below, we define a *onehot_encoder()* function to convert an integer to a one-hot variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3dc93135",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def onehot_encoder(position,depth):\n",
    "    onehot=torch.zeros((depth,))\n",
    "    onehot[position]=1\n",
    "    return onehot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4552a9",
   "metadata": {},
   "source": [
    "The function takes two arguments: the first argumetn *position* is the index at which the value is turned on as 1, and the second argument *depth* is the length of the one-hot variable. \n",
    "\n",
    "For example, if we print out the value of *onehot_encoder(1,5)*, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bbb1c2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "print(onehot_encoder(1,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4498803a",
   "metadata": {},
   "source": [
    "The result shows a five-value tensor with the second place as 1 and the rest 0s. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e0b2e3",
   "metadata": {},
   "source": [
    "## 1.2. Convert An Integer to A Pair of One-Hots\n",
    "Since the pattern we are trying to create are multiples of five, we'll denote an integer as a pair of two numbers: the quotient and the remainder. Any number can be represented as $quotient\\times5+remainder$. Since we are dealing with an integer between 0 and 99, the value of $quotient$ ranges from 0 to 19 and the value of $remainder$ ranges from 0 to 4. We'll use a 20-value one-hot variable to represent the quotient and a 4-value one-hot variable to represent the remainder. For simplicity, we'll concetenate the two one-hot variables into a single 25-value tensor, with the 20-value one-hot variable followed by the 5-value one-hot variable. \n",
    "\n",
    "The function *int_to_onehots()* is defined as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4823d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def int_to_onehots(number):\n",
    "    # calculate the quotient\n",
    "    quotient=number//5\n",
    "    # calculate the remainder\n",
    "    remainder=number%5\n",
    "    # convert to onehots\n",
    "    onehot_quotient=onehot_encoder(quotient,20)\n",
    "    onehot_remainder=onehot_encoder(remainder,5)\n",
    "    # concatenate\n",
    "    combined=torch.cat([onehot_quotient,onehot_remainder])\n",
    "    return combined"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b978d3",
   "metadata": {},
   "source": [
    "The argument to the function *int_to_onehots()* is an integer between 0 and 99. The function first calculates the quotient and the remainder. It then converts the quotient and remainder into two one-hot variables. Finally, it concatenate the two one-hot variables into a 25-value PyTorch tensor as output. \n",
    "\n",
    "Let's use the function to convert 75 to two one-hot variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a248270b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "        0., 0., 1., 0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "onehots75=int_to_onehots(75)\n",
    "print(onehots75)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a9c851",
   "metadata": {},
   "source": [
    "The result is a 25-value tensor. The first 20 values of the tensor is a one-hot variable with the 16th place turned on as 1, and the last 5 values of the tensor is a one-hot variable with the first place turned on as 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f1f903",
   "metadata": {},
   "source": [
    "## 1.3. Convert A Pair of One-Hots into An Integer\n",
    "To function *int_to_onehots()* converts an integer into two one-hot variables, concatenated together. In a way, the function is translate human language into machine language. \n",
    "\n",
    "Next, we want to translate machine language back to huamn language. Suppose we have two one-hot variables concatenated together, how can we convert them into an integer? The following function *onehots_to_int()* accomplishes that goal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91129836",
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehots_to_int(onehots):\n",
    "    # extract quotient and remainder\n",
    "    onehot_quotient=onehots[:20]\n",
    "    onehot_remainder=onehots[-5:]    \n",
    "    quotient=torch.argmax(onehot_quotient)\n",
    "    remainder=torch.argmax(onehot_remainder)\n",
    "    # concatenate\n",
    "    number=5*quotient+remainder\n",
    "    return number.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22cd343",
   "metadata": {},
   "source": [
    "The function *int_to_onehots()* takes an argument *onehots*, which is the concatenated one-hot variables from the quotient and the remainder. The function then converts the one-hot variables into quotient and remainder. From there, the function calculates the value of the number as $quotient\\times5+remainder$. \n",
    "\n",
    "Let's test the function to see what happens if we use the tensor *onehots75* we just created as the input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e5da6b0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75\n"
     ]
    }
   ],
   "source": [
    "print(onehots_to_int(onehots))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d6ad2c",
   "metadata": {},
   "source": [
    "The result shows the function converts the one-hot variables to an integer 75, which is the right answer. So the functions are defined properly. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bcbdf00",
   "metadata": {},
   "source": [
    "# 2.  A GAN to Generate Patterns\n",
    "Our goal is to train a model so that the generator can generate a sequence of 10 integers, all multiples of five. We first generate the training data, and then convert them to machine language in batches. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbe5b7f",
   "metadata": {},
   "source": [
    "## 2.1. Generate A Batch of Training Data\n",
    "For simplicity, we'll generate a sequence of 10 integers. We'll then convert the sequence into ten machine readable numbers. That is, we'll convert the sequence to ten 25-value tensors. \n",
    "\n",
    "The function below generates a sequence of 10 integers, all multiples of five:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51ad3ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def gen_sequence():\n",
    "    indices = random.sample(range(10), 10)\n",
    "    values = torch.tensor(indices)*5\n",
    "    return values    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9309ed21",
   "metadata": {},
   "source": [
    "We first use the *sample()* method in the *random* library to generate ten numbers between 0 ane 9. We then convert the ten numbers into PyTorch tensors and multiply them by five. \n",
    "\n",
    "Let's try to generate a sequence of training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1bfa571b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([45,  0,  5, 40, 35, 10, 20, 30, 25, 15])\n"
     ]
    }
   ],
   "source": [
    "sequence=gen_sequence()\n",
    "print(sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af305baa",
   "metadata": {},
   "source": [
    "The values are all multiples of five. Next, we convert each number to a pair of one-hot variables so that we can feed them to the neural network later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "567e9003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "def gen_batch():\n",
    "    sequence=gen_sequence()\n",
    "    batch=[int_to_onehots(i).numpy() for i in sequence]\n",
    "    return torch.tensor(batch)\n",
    "batch=gen_batch()\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7598bc",
   "metadata": {},
   "source": [
    "The function *gen_batch()* creates a batch of ten 25-value tensors to feed to the neural network for training purpose. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36e951c",
   "metadata": {},
   "source": [
    "We also define a function *data_to_num()* to convert concatenated one-hot variables to a sequence of integers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06773658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0,  5, 40, 45, 10, 20, 30, 25, 35, 15])\n"
     ]
    }
   ],
   "source": [
    "def data_to_num(data):\n",
    "    multiple=torch.argmax(data[:,:20],dim=-1)\n",
    "    remainder=torch.argmax(data[:,20:],dim=-1)\n",
    "    num=multiple*5+remainder\n",
    "    return num\n",
    "numbers=data_to_num(batch)\n",
    "print(numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8c5b19",
   "metadata": {},
   "source": [
    "Next, we'll create two neural networks: one for the discriminator D and one for the generator G. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73036c27",
   "metadata": {},
   "source": [
    "## 2.2. The Discriminator D\n",
    "The discriminator D is a binary classifier, which is very similar to the binary classifier for clothes items we discussed in Chapter 2. Here the discriminator's job is to classify the samples into either real or fake. \n",
    "\n",
    "We use PyTorch to create the following discriminator neural network D, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc2f8827",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# determine the device automatically\n",
    "device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# the discriminator D is a binary classifier\n",
    "D=nn.Sequential(\n",
    "    nn.Linear(25,1),\n",
    "    nn.Sigmoid()).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25216c46",
   "metadata": {},
   "source": [
    "The input size is 25 so we use 25 as the input size in the first Linear layer in the model. The hidden layer has 64 neurons. The output layer has just one neuron in it: the output of the discriminator D is a single value. We use the Sigmoid activation function to squeeze the output to the range [0, 1] so it can be interpreted as the probability that the sample is real. With complementary probability 1-p, the sample is fake. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db511f44",
   "metadata": {},
   "source": [
    "## 2.3. The Generator G\n",
    "The generator G's job is to create a sequence of numbers so that they can pass as real in front of the discriminator D. That is, G is trying to create a sequence of numbers to maximize the probability that D thinks the numbers are from the training dataset.\n",
    "\n",
    "We create the following neural network to respresent the generator G:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8a6a600a",
   "metadata": {},
   "outputs": [],
   "source": [
    "G=nn.Sequential(\n",
    "    nn.Linear(100,25),\n",
    "    nn.ReLU()).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4afb7be7",
   "metadata": {},
   "source": [
    "We'll feed random data from a 100-dimensional latent space, $(z_1, z_2, ..., z_100)$, to the generator. The generator then generates a tensor of 25 values based on the input from the latent space. Note here we use the ReLU activation function at the output layer so the output are all nonnegative. Since we are trying to generate index values, nonnegative values are appropriate here. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02df292",
   "metadata": {},
   "source": [
    "## 2.4. Optimizers and the Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b56e69",
   "metadata": {},
   "source": [
    "The loss function is binary cross-entropy loss. The discriminator D is trying to maximize the accuracy of the binary classification: identify a real sample as real and a fake sample as fake. The generator, on the other hand, is trying to minimize the probability that the fake sample is being identified as fake. \n",
    "\n",
    "We'll use the Adam optimizer for both the discriminator and the generator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "62249b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn=nn.BCELoss()\n",
    "lr=0.001\n",
    "optimD=torch.optim.Adam(D.parameters(),lr=lr)\n",
    "optimG=torch.optim.Adam(G.parameters(),lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e27a43",
   "metadata": {},
   "source": [
    "# 3. Train and Use the Model\n",
    "Now that we have the training data and two networks, we'll train the model. After that, we'll discard the discriminator and use the generator to generate a sequence of ten integers. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd8eed1",
   "metadata": {},
   "source": [
    "## 3.1. Train the Model\n",
    "We first create labels of zeros and ones as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1579eddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_labels=torch.ones((10,1)).to(device)\n",
    "fake_labels=torch.zeros((10,1)).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f26be1",
   "metadata": {},
   "source": [
    "The real labels are all ones, while the fake labels are all zeros. We'll compare these labels with the predictions from the discriminator to train both the discriminator and the generator. \n",
    "\n",
    "We train the model for 10000 epochs, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fe819235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([35, 58, 15, 65, 50, 18, 53, 37, 19, 16], device='cuda:0')\n",
      "tensor([11, 17, 86, 46, 54, 65, 46, 45,  9, 65], device='cuda:0')\n",
      "tensor([17,  5, 15, 67,  6, 15, 17, 85, 85, 10], device='cuda:0')\n",
      "tensor([ 5, 15, 15, 47, 15,  5, 17, 10, 25, 15], device='cuda:0')\n",
      "tensor([25, 45, 10,  5, 45,  5, 45, 15, 50, 47], device='cuda:0')\n",
      "tensor([40,  0, 35, 35, 15, 25,  0, 40,  5,  0], device='cuda:0')\n",
      "tensor([35, 40, 35, 40,  0, 40, 35, 30, 35, 40], device='cuda:0')\n",
      "tensor([20,  0,  0,  0, 25, 30,  0,  0, 35, 20], device='cuda:0')\n",
      "tensor([ 0,  0, 10, 20,  0, 10, 35, 35, 30,  0], device='cuda:0')\n",
      "tensor([10, 10, 30, 30,  0,  0, 10, 10, 45, 10], device='cuda:0')\n",
      "tensor([10, 30, 30, 10,  5, 10,  0, 30, 45,  5], device='cuda:0')\n",
      "tensor([10, 10, 25, 25,  5, 45, 25, 10, 25, 10], device='cuda:0')\n",
      "tensor([40,  5, 45, 35, 25,  5, 15, 25, 25, 40], device='cuda:0')\n",
      "tensor([45, 25, 35, 35, 35, 40, 40, 15, 20, 15], device='cuda:0')\n",
      "tensor([20,  0, 20, 35, 20, 20, 35, 20,  0, 40], device='cuda:0')\n",
      "tensor([ 0, 35, 35, 35,  0, 20,  0, 30,  0, 15], device='cuda:0')\n",
      "tensor([30,  0,  0, 30,  0,  0, 30, 30, 10,  0], device='cuda:0')\n",
      "tensor([10,  0, 10, 10,  0, 10, 30, 10,  0, 10], device='cuda:0')\n",
      "tensor([10,  5, 10, 40, 45,  5, 10, 45, 10, 10], device='cuda:0')\n",
      "tensor([40, 45, 40, 25, 10, 45,  0, 45, 45, 10], device='cuda:0')\n",
      "tensor([ 5, 45,  5, 25, 20, 10, 45, 40, 45, 40], device='cuda:0')\n",
      "tensor([40, 20,  5, 40,  5, 25, 25,  5, 35, 35], device='cuda:0')\n",
      "tensor([15, 35, 25, 35, 35, 20, 15, 35, 15, 35], device='cuda:0')\n",
      "tensor([15, 30,  0, 35, 15, 15, 30,  0, 15, 30], device='cuda:0')\n",
      "tensor([30, 45, 30,  0, 30,  0,  0, 15, 15, 30], device='cuda:0')\n",
      "tensor([ 0, 30,  0,  0, 10,  0,  0, 30,  0,  0], device='cuda:0')\n",
      "tensor([20, 30, 30, 30,  0,  5, 45, 30, 10,  0], device='cuda:0')\n",
      "tensor([10, 10, 45, 10, 30, 40, 45,  0,  5, 10], device='cuda:0')\n",
      "tensor([40,  5, 40, 10,  5, 40, 40, 40, 40, 30], device='cuda:0')\n",
      "tensor([45, 20, 45, 25, 45,  0, 20,  5, 40, 40], device='cuda:0')\n",
      "tensor([40, 35, 25, 45, 45, 35, 35, 35, 20, 20], device='cuda:0')\n",
      "tensor([15, 20, 20, 40, 25,  0, 35, 15, 20, 25], device='cuda:0')\n",
      "tensor([35, 20,  0, 20, 15,  0, 35, 20, 35, 35], device='cuda:0')\n",
      "tensor([ 0, 30, 15,  5, 15, 25, 15,  0, 20, 15], device='cuda:0')\n",
      "tensor([ 0, 15,  0, 35, 30, 30, 15, 15, 15,  0], device='cuda:0')\n",
      "tensor([30,  5,  0, 45,  5, 30, 30, 10,  5,  0], device='cuda:0')\n",
      "tensor([10,  0, 45, 40, 10, 45, 45, 40,  0, 10], device='cuda:0')\n",
      "tensor([45, 30, 45,  5,  5, 45, 45,  5, 25, 40], device='cuda:0')\n",
      "tensor([ 5, 20, 30, 40, 10, 20, 20, 20,  5, 20], device='cuda:0')\n",
      "tensor([35, 35, 20, 20, 20,  0, 35, 35, 25, 20], device='cuda:0')\n",
      "tensor([20,  0, 25, 40, 35, 15, 15, 25, 15, 35], device='cuda:0')\n",
      "tensor([25, 20, 25, 35, 35, 20, 20, 35, 20,  5], device='cuda:0')\n",
      "tensor([45, 15, 15,  0, 15, 30, 30, 15, 35, 25], device='cuda:0')\n",
      "tensor([ 0, 15, 15, 30, 45, 15,  0,  0, 15, 45], device='cuda:0')\n",
      "tensor([10,  0, 15, 45, 45, 30, 15, 10, 45, 10], device='cuda:0')\n",
      "tensor([10,  0, 45, 45, 40,  5,  0,  0,  5, 10], device='cuda:0')\n",
      "tensor([40, 40,  0,  5,  5, 40, 10, 45,  5, 40], device='cuda:0')\n",
      "tensor([ 5, 25, 20,  5, 25,  5, 25, 40, 20,  5], device='cuda:0')\n",
      "tensor([25, 20, 20, 20, 20,  0, 25,  0, 25, 20], device='cuda:0')\n",
      "tensor([35, 20, 35, 35, 20, 20, 35, 35, 20, 20], device='cuda:0')\n",
      "tensor([15, 25, 30, 20, 15, 40, 25, 35, 35, 20], device='cuda:0')\n",
      "tensor([30, 35, 30, 15, 15, 15, 15, 45, 35, 35], device='cuda:0')\n",
      "tensor([ 0, 35, 15, 45, 30,  0,  0, 45, 10, 30], device='cuda:0')\n",
      "tensor([45, 45, 40, 15, 30,  0, 15, 45,  0,  0], device='cuda:0')\n",
      "tensor([ 5, 45, 40,  0, 45,  5, 40, 40, 45, 40], device='cuda:0')\n",
      "tensor([ 5,  0,  0, 40, 40,  5,  5,  0, 40, 45], device='cuda:0')\n",
      "tensor([20,  0,  0, 20,  5,  0,  0,  5, 40,  0], device='cuda:0')\n",
      "tensor([25,  0, 20,  5,  0, 35,  5, 40,  5, 35], device='cuda:0')\n",
      "tensor([40, 35, 35, 30, 25, 20, 10, 25, 35, 20], device='cuda:0')\n",
      "tensor([20, 30, 35, 35, 25, 25, 30, 35, 10, 35], device='cuda:0')\n",
      "tensor([40, 45, 45, 10, 45, 30, 30, 45, 30, 10], device='cuda:0')\n",
      "tensor([ 0, 15, 30, 35, 10, 30, 35, 30, 10, 45], device='cuda:0')\n",
      "tensor([30, 40, 15, 15, 30, 15,  0, 10, 40, 10], device='cuda:0')\n",
      "tensor([40, 45, 15, 15,  0,  0, 40, 15,  5,  5], device='cuda:0')\n",
      "tensor([ 0,  5,  5,  5,  0,  0, 40,  0, 40, 40], device='cuda:0')\n",
      "tensor([ 5,  5, 45,  5,  5,  5,  5,  5,  5, 20], device='cuda:0')\n",
      "tensor([ 5, 20, 25, 20, 25,  0,  5,  5, 20, 30], device='cuda:0')\n",
      "tensor([20, 20,  0, 25, 35, 10, 20, 25, 20, 25], device='cuda:0')\n",
      "tensor([35, 25, 35, 35, 30, 30, 35, 35, 15, 25], device='cuda:0')\n",
      "tensor([45, 30, 45, 30, 30, 35, 35, 30, 10, 35], device='cuda:0')\n",
      "tensor([35, 45, 15, 45, 10, 35,  0,  0, 30, 35], device='cuda:0')\n",
      "tensor([45, 15, 40, 15, 15, 40, 10, 30, 10, 15], device='cuda:0')\n",
      "tensor([40,  0,  0,  0, 30,  0, 40,  0,  0, 40], device='cuda:0')\n",
      "tensor([40, 15, 15,  5,  5,  5,  0,  0,  0, 15], device='cuda:0')\n",
      "tensor([ 5, 25, 20, 15, 23, 20,  0,  0,  5,  0], device='cuda:0')\n",
      "tensor([20, 25, 20, 25,  0, 25, 25, 20, 25,  5], device='cuda:0')\n",
      "tensor([30, 25, 25,  5, 25, 25, 20, 20, 25,  0], device='cuda:0')\n",
      "tensor([10, 45, 10, 45, 30, 45, 30, 45, 10, 25], device='cuda:0')\n",
      "tensor([35,  5, 45, 40, 35, 40,  0, 35, 20, 20], device='cuda:0')\n",
      "tensor([35, 40, 10, 30, 45, 35, 45, 40, 40, 15], device='cuda:0')\n",
      "tensor([35, 40, 40,  5, 35, 15, 35,  0, 35, 45], device='cuda:0')\n",
      "tensor([15, 15, 10,  0, 30, 15,  0, 40,  0, 40], device='cuda:0')\n",
      "tensor([ 0,  0, 40,  0,  0, 40, 25, 20, 40, 40], device='cuda:0')\n",
      "tensor([20, 40, 30,  0, 25,  5,  0, 30,  0,  0], device='cuda:0')\n",
      "tensor([ 5, 25, 25,  0, 35, 20, 25,  0,  5,  5], device='cuda:0')\n",
      "tensor([ 5, 20,  5, 20, 20, 10,  5, 10, 45, 10], device='cuda:0')\n",
      "tensor([10,  5, 25, 20,  5,  0, 45, 10,  5, 25], device='cuda:0')\n",
      "tensor([35,  0,  5,  5, 15, 45,  5,  0, 30, 45], device='cuda:0')\n",
      "tensor([ 0,  0,  5, 15, 45, 15, 40, 15, 15, 45], device='cuda:0')\n",
      "tensor([40, 40, 35,  0, 40, 15, 15, 15, 15, 35], device='cuda:0')\n",
      "tensor([30,  0, 25, 40, 30,  0, 15, 25, 35, 35], device='cuda:0')\n",
      "tensor([25, 30, 45, 20, 35,  0, 35,  0, 20, 35], device='cuda:0')\n",
      "tensor([ 0, 10,  0, 25, 15, 10, 10, 30, 30, 10], device='cuda:0')\n",
      "tensor([ 5, 35,  0, 20, 20,  0, 20,  5, 10, 10], device='cuda:0')\n",
      "tensor([20, 10,  5, 45,  5, 25, 20,  5, 20, 30], device='cuda:0')\n",
      "tensor([ 5,  0, 45,  5,  0, 20, 20, 45,  0, 45], device='cuda:0')\n",
      "tensor([ 0, 45, 40, 45, 40, 45, 40, 45, 40, 45], device='cuda:0')\n",
      "tensor([45, 30, 40, 20, 25, 35, 35, 35, 40, 45], device='cuda:0')\n",
      "tensor([40, 15, 45, 45, 45, 30, 35, 35, 40, 15], device='cuda:0')\n",
      "tensor([30, 25, 15,  0, 35, 10, 25, 35, 40, 40], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for i in range(10000):\n",
    "    # train discriminator on real data\n",
    "    gloss=dloss=0\n",
    "    # Generate examples of even real data\n",
    "    true_data=gen_batch().to(device)\n",
    "    # use 1 as labels since they are real\n",
    "    preds=D(true_data)\n",
    "    loss_D=loss_fn(preds,real_labels)\n",
    "    optimD.zero_grad()\n",
    "    loss_D.backward()\n",
    "    optimD.step()\n",
    "    dloss+=loss_D    \n",
    "    # train D on fake data\n",
    "    noise=torch.randn(10,100).to(device)\n",
    "    generated_data=G(noise)\n",
    "    # use 0 as labels since they are fake\n",
    "    preds=D(generated_data)\n",
    "    loss_D=loss_fn(preds,fake_labels)\n",
    "    optimD.zero_grad()\n",
    "    loss_D.backward()\n",
    "    optimD.step()\n",
    "    dloss+=loss_D      \n",
    "    # train G \n",
    "    noise=torch.randn(10,100).to(device)\n",
    "    generated_data=G(noise)\n",
    "    # use 1 as labels since G wants to fool D\n",
    "    preds=D(generated_data)\n",
    "    loss_G=loss_fn(preds,real_labels)\n",
    "    optimG.zero_grad()\n",
    "    loss_G.backward()\n",
    "    optimG.step()\n",
    "    gloss+=loss_G      \n",
    "    if i % 100 == 0:\n",
    "        print(data_to_num(generated_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f7afb2",
   "metadata": {},
   "source": [
    "In each iteration, we generate a batch of ten numbers. We first train the discriminator using the real samples. After that, the generator creates a batch of fake samples and we use them to train the discriminator D again. Finally, we let the generator create a batch of fake samples again, but we use them to train the generator instead.\n",
    "\n",
    "In the first few hundred epochs, the generator still generates numbers that are not multiples of five. But after a few hundred epochs, all the numbers generated are multiples of five. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441286c7",
   "metadata": {},
   "source": [
    "## 3.2. Use the Trained Model\n",
    "We'll discard the discriminator and save the trained generator on the local folder, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "252eb61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to TorchScript\n",
    "scripted = torch.jit.script(G) \n",
    "# Save the model\n",
    "import os\n",
    "os.makedirs(\"files/ch04\", exist_ok=True)\n",
    "scripted.save('files/ch04/num_gen.pt') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9bc75d",
   "metadata": {},
   "source": [
    "We have now saved the generator to the local folder. To use the generator, we don't even need to define the model. We simply load up the model and use it to generate data points as follows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "96e2ab86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RecursiveScriptModule(\n",
       "  original_name=Sequential\n",
       "  (0): RecursiveScriptModule(original_name=Linear)\n",
       "  (1): RecursiveScriptModule(original_name=ReLU)\n",
       ")"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_G=torch.jit.load('files/ch04/num_gen.pt',\n",
    "                     map_location=device)\n",
    "new_G.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37fd191b",
   "metadata": {},
   "source": [
    "The generator is now reloaded. We can use it to generate a sequence of ten integers as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9f298794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([25,  0, 30, 40, 25, 35, 10, 30, 10,  0], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# obtain inputs from the latent space\n",
    "noise=torch.randn((10,100)).to(device)\n",
    "# feed the input to the generator \n",
    "new_data=new_G(noise) \n",
    "print(data_to_num(new_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27f8088",
   "metadata": {},
   "source": [
    "The generated numbers are all multiples of five. So the trained model really works! \n",
    "\n",
    "You can easily change the code to generate other patterns such as odd numbers, even numbers, multiples of three, and so on. I'll leave that as an exercise for you."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

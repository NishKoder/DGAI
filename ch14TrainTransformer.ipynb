{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "653089c9",
   "metadata": {},
   "source": [
    "# Chapter 14: Train A Transformer to Generate Text\n",
    "\n",
    "\n",
    "\n",
    "The GPT2 transformer we created in Chapter 3 is a large language model with 1.5 billion parameters. Once we load the pretrained weights, the model generates text that are as good as human-written. However, training such a large language model requires supercomputing resources, which most people don't have. \n",
    "\n",
    "In this chapter, we'll build a much smaller transformer and train it from scratch by using the Old Man and the Sean text that we used in Chapter 12. You'll see that the trained model generates better text than the LSTM model that we used in Chapter 12. \n",
    "\n",
    "The main purpose of this chapter is to learn how to build a language model from scratch and train it using data and use it to generate text. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d1c19a",
   "metadata": {},
   "source": [
    "Start a new cell in ch14.ipynb and execute the following lines of code in it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "117477d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.makedirs(\"files/ch14\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c268aecc",
   "metadata": {},
   "source": [
    "# 1. Tokenization with Torchtext\n",
    "We built our own vocabulary and word indexes from scratch in Chapter 12 based on a raw text file. The experience shows the steps involved in word tokenization. \n",
    "\n",
    "In this chapter, we'll use the built-in tokenizer in the Torchtext library. We'll use the raw text file of The Old Man and the Sea by Ernest Hemingway. Go to https://archive.org/stream/TheOldManAndTheSea-Eng-Ernest/oldmansea_djvu.txt and download the raw text file from the website and save it as *oldmansea.txt* in the folder /Desktop/ai/files/ch14/ on your computer. Make sure your remove the top and bottom paragraphs that are not part of the original book. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed84eadd",
   "metadata": {},
   "source": [
    "## 1.1. Clean Up the Text\n",
    "Take a look at the raw text file you just saved. You'll notice that we need to remove some information from the file. For example, at the top of each section, it has the section number such as -24- in it. It also includes \"The Old Man and the Sea Asiaing.com\" after the section number. Further, it has page numbers such as \"[117] in it. We therefore load the raw text file and remove certain information from the file, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbfb8bd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  He was an old man who fished alone in a skiff in the Gulf Stream and he had gone  eighty four days now without taking a fish. In the first forty days a boy had been with him.  But after forty days without a fish the boy's parents had told him that the old man was  now definitely and finally salao, which is the worst form of unlucky, and the boy had gone  at their orders in another boat which caught three good fish the first week. It made the  boy sad to see the old man come in each day with his skiff empty and he always went  down to help him carry either the coiled lines or the gaff and harpoon and the sail that  was furled around the mast. The sail was patched with flour sacks and, furled, it looked  like the flag of permanent defeat.   The old man was thin and gaunt with deep wrinkles in the back of his neck. The  brown blotches of the benevolent skin cancer the sun brings from its  reflection on the  tropic sea were on his cheeks. The blotches ran well down the sides of his face \n"
     ]
    }
   ],
   "source": [
    "with open(\"files/ch14/oldmansea.txt\",\"r\") as f:\n",
    "    text=f.read()\n",
    "text=text.replace(\"The Old Man and the Sea\", \"\")    \n",
    "text=text.replace(\"Asiaing.com\", \"\")   \n",
    "text=text.replace(\"/'\", '\"') \n",
    "text=text.replace(\"\\n\", ' ') \n",
    "for x in \"0123456789]-[\":\n",
    "    text=text.replace(f\"{x}\", \"\")\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d073ca",
   "metadata": {},
   "source": [
    "The line break (\\n) is treated as part of the text, so we have replaced line breaks with white spaces. We'll use the Torchtext tokenizer later, which will automatically converts upper case letters to lower cases and separate punctuations from words. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce10d036",
   "metadata": {},
   "source": [
    "We can now save the cleaned up text as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0db886ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"files/ch14/cleaned_text.txt\",\"w\") as f:\n",
    "    f.write(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df377e7a",
   "metadata": {},
   "source": [
    "## 1.2. Torchtext Tokenizer\n",
    "First, we load up the clean text file that we just saved. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79083fdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '', 'He', 'was', 'an', 'old', 'man', 'who', 'fished', 'alone', 'in', 'a', 'skiff', 'in', 'the', 'Gulf', 'Stream', 'and', 'he', 'had', 'gone', '', 'eighty', 'four', 'days', 'now', 'without', 'taking', 'a', 'fish.', 'In', 'the', 'first', 'forty', 'days', 'a', 'boy', 'had', 'been', 'with', 'him.', '', 'But', 'after', 'forty', 'days', 'without', 'a', 'fish', 'the', \"boy's\", 'parents', 'had', 'told', 'him', 'that', 'the', 'old', 'man', 'was', '', 'now', 'definitely', 'and', 'finally', 'salao,', 'which', 'is', 'the', 'worst', 'form', 'of', 'unlucky,', 'and', 'the', 'boy', 'had', 'gone', '', 'at', 'their', 'orders', 'in', 'another', 'boat', 'which', 'caught', 'three', 'good', 'fish', 'the', 'first', 'week.', 'It', 'made', 'the', '', 'boy', 'sad', 'to']\n"
     ]
    }
   ],
   "source": [
    "with open(\"files/ch14/cleaned_text.txt\",\"r\") as f:\n",
    "    text=f.read()\n",
    "words=text.split(\" \") \n",
    "print(words[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5b8bb8",
   "metadata": {},
   "source": [
    "We then import the Torchtext tokenizer as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c57af8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from collections import Counter\n",
    "from torchtext.vocab import vocab\n",
    "\n",
    "tokenizer = get_tokenizer('basic_english')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7f5710",
   "metadata": {},
   "source": [
    "We create a vocabulary item *vocab* based on the words in the text file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d463840",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = Counter()\n",
    "for word in words:\n",
    "    counter.update(tokenizer(word))\n",
    "vocabulary = vocab(counter, min_freq=1,\n",
    "   specials=('\\<unk\\>', '\\<BOS\\>', '\\<EOS\\>', '\\<PAD\\>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece913b9",
   "metadata": {},
   "source": [
    "To use the vocabulary, we can convert an example sentence below to indexes, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f06b6e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[271, 47, 13, 522, 68, 28]\n"
     ]
    }
   ],
   "source": [
    "example=\"Today is a great day.\"\n",
    "idx=[]\n",
    "for w in example.split():\n",
    "    idx += vocabulary(tokenizer(w))\n",
    "print(idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fccf648d",
   "metadata": {},
   "source": [
    "We can also use the *lookup_token()* method to convert indexes back to text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dbbd25de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "today is a great day .\n"
     ]
    }
   ],
   "source": [
    "txt=[vocabulary.lookup_token(i) for i in idx]\n",
    "print(\" \".join(txt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede2b9bd",
   "metadata": {},
   "source": [
    "So the vocabulary object we built works properly. We'll use these methods later in the chapter to generate text by converting generated integer numbers back to text. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e0b2e3",
   "metadata": {},
   "source": [
    "## 1.2. Create Batches for Training\n",
    "We first create a PyTorch dataset based on the text file *cleaned_text.txt* and the vocabulary object we created in the last subsection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02e58aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([29725])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "data=[torch.tensor(vocabulary(tokenizer(w)),\n",
    "         dtype=torch.long) for w in words]\n",
    "data=torch.cat(tuple(filter(lambda t:t.numel()>0,data)))\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f1f903",
   "metadata": {},
   "source": [
    "The text file is converted into 29,725 indexes. We'll break the data into smaller sequences: each sequence will have 256 words in it and each batch contains 32 sequences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8bafe4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "seq_len = 256\n",
    "\n",
    "batches=[]\n",
    "i=0\n",
    "while True:\n",
    "    x=data[i:i+seq_len]\n",
    "    y=data[i+1:i+seq_len+1]\n",
    "    batches.append((x,y))\n",
    "    i+=1\n",
    "    if i+seq_len+1>=len(data):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30a59780",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 32\n",
    "loader=DataLoader(batches,batch_size=batch_size,\n",
    "                  shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffddc678",
   "metadata": {},
   "source": [
    "We'll print out one batch and examine the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6943ca3a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 256]) torch.Size([32, 256])\n"
     ]
    }
   ],
   "source": [
    "x,y=next(iter(loader))\n",
    "print(x.shape,y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a85aa66",
   "metadata": {},
   "source": [
    "The above results indicate that if you shift x one position to the right, you have y. That's exactly what we intend to do. We'll use x as features and y as targets. By using the above training data, the model learns to predict the next word based on the prompt. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e27a43",
   "metadata": {},
   "source": [
    "# 2. Build and Train the Transformer\n",
    "We'll build an encoder transformer from scratch and train it by using the data we prepared in the last section. \n",
    "\n",
    "## 2.1. The Encoder Transformer \n",
    "We first define the *PositionalEncoding()* class based on the definition provided by PyTroch documentation site: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e8487434",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from torch.nn import TransformerEncoder,TransformerEncoderLayer\n",
    "from torch.utils.data import dataset\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout = 0.1, max_len = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2)\\\n",
    "                             * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cef75e8",
   "metadata": {},
   "source": [
    "We define the hyperparameters we use in the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3c1f30d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# size of vocabulary\n",
    "ntokens = len(vocabulary) \n",
    "# embedding dimension\n",
    "emsize = 256\n",
    "# dimension of the feedforward network\n",
    "d_hid = 256   \n",
    "nlayers = 2 \n",
    "nhead = 2  \n",
    "dropout = 0.2  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb3b029",
   "metadata": {},
   "source": [
    "The encoder transformer is defined as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "304323b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, ntoken, d_model, nhead, d_hid,\n",
    "                 nlayers, dropout=0.5):\n",
    "        super().__init__() \n",
    "        self.model_type=\"Transformer\"\n",
    "        self.pos_encoder=PositionalEncoding(d_model,dropout)\n",
    "        encoder_layers = TransformerEncoderLayer(d_model,\n",
    "                                 nhead, d_hid, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(\n",
    "            encoder_layers, nlayers)\n",
    "        self.embedding=nn.Embedding(ntoken,d_model)\n",
    "        self.d_model=d_model\n",
    "        self.linear=nn.Linear(d_model,ntoken)\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.linear.bias.data.zero_()\n",
    "        self.linear.weight.data.uniform_(-initrange, initrange)        \n",
    "\n",
    "    def forward(self,src):\n",
    "        src_mask=nn.Transformer.generate_square_subsequent_mask(\n",
    "            src.shape[1])\n",
    "        src=self.embedding(src)*math.sqrt(self.d_model)\n",
    "        src=self.pos_encoder(src)\n",
    "        output=self.transformer_encoder(src,src_mask)\n",
    "        output=self.linear(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56d0e1a",
   "metadata": {},
   "source": [
    "## 2.2. Create the Model\n",
    "We first instantiate a model as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0d429629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (pos_encoder): PositionalEncoding(\n",
      "    (dropout): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      "  (transformer_encoder): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-1): 2 x TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "        (linear2): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.2, inplace=False)\n",
      "        (dropout2): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (embedding): Embedding(2532, 256)\n",
      "  (linear): Linear(in_features=256, out_features=2532, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "model=Model(ntokens,emsize,nhead,d_hid,nlayers,dropout)\n",
    "model=model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc02d1aa",
   "metadata": {},
   "source": [
    "The optimizer and the loss function are as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8c746b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=0.0001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d561161b",
   "metadata": {},
   "source": [
    "We'll train the Model next."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd8eed1",
   "metadata": {},
   "source": [
    "# 3. Train the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d94cc0",
   "metadata": {},
   "source": [
    "We then train the model for 100 epochs, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2ac8080b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 loss 4.291328947396543\n",
      "epoch 2 loss 3.286152108355014\n",
      "epoch 3 loss 3.1568637826155372\n",
      "epoch 4 loss 3.1182432759727123\n",
      "epoch 5 loss 3.098952170691972\n",
      "epoch 6 loss 3.0861858009127143\n",
      "epoch 7 loss 3.0772623860486554\n",
      "epoch 8 loss 3.0703098667819906\n",
      "epoch 9 loss 3.064477465181216\n",
      "epoch 10 loss 3.0598204001282765\n",
      "epoch 11 loss 3.0560653173444585\n",
      "epoch 12 loss 3.0522191242338135\n",
      "epoch 13 loss 3.049309714738761\n",
      "epoch 14 loss 3.0470319146312668\n",
      "epoch 15 loss 3.044534145297238\n",
      "epoch 16 loss 3.0425154681314472\n",
      "epoch 17 loss 3.040149181078104\n",
      "epoch 18 loss 3.0384829846836716\n",
      "epoch 19 loss 3.0369212252050475\n",
      "epoch 20 loss 3.0358088901842843\n",
      "epoch 21 loss 3.034229785171575\n",
      "epoch 22 loss 3.0324920336923173\n",
      "epoch 23 loss 3.0317186941670804\n",
      "epoch 24 loss 3.030616597166279\n",
      "epoch 25 loss 3.0292223848556206\n",
      "epoch 26 loss 3.028349311033367\n",
      "epoch 27 loss 3.0273971511020723\n",
      "epoch 28 loss 3.026491512822535\n",
      "epoch 29 loss 3.0256204840673555\n",
      "epoch 30 loss 3.0248236821865286\n",
      "epoch 31 loss 3.0241274559277795\n",
      "epoch 32 loss 3.023457092519174\n",
      "epoch 33 loss 3.0224490541070863\n",
      "epoch 34 loss 3.022064645179061\n",
      "epoch 35 loss 3.021191551682745\n",
      "epoch 36 loss 3.0208311904136833\n",
      "epoch 37 loss 3.0199931641225577\n",
      "epoch 38 loss 3.019253578558807\n",
      "epoch 39 loss 3.0190953985746467\n",
      "epoch 40 loss 3.018425372731543\n",
      "epoch 41 loss 3.0178621514224075\n",
      "epoch 42 loss 3.0172945593648572\n",
      "epoch 43 loss 3.0171207812140484\n",
      "epoch 44 loss 3.0163620061905454\n",
      "epoch 45 loss 3.0158954821243866\n",
      "epoch 46 loss 3.0155923656480192\n",
      "epoch 47 loss 3.01504495037754\n",
      "epoch 48 loss 3.014646429451229\n",
      "epoch 49 loss 3.014107601132636\n",
      "epoch 50 loss 3.0139233033121213\n",
      "epoch 51 loss 3.0134295511193954\n",
      "epoch 52 loss 3.0132376697242065\n",
      "epoch 53 loss 3.0129792584140707\n",
      "epoch 54 loss 3.012553574337374\n",
      "epoch 55 loss 3.011979097134386\n",
      "epoch 56 loss 3.011789665677778\n",
      "epoch 57 loss 3.011644927997154\n",
      "epoch 58 loss 3.0109977874900826\n",
      "epoch 59 loss 3.0106277719512176\n",
      "epoch 60 loss 3.010429253925071\n",
      "epoch 61 loss 3.010324375378322\n",
      "epoch 62 loss 3.010074781672574\n",
      "epoch 63 loss 3.0093696148465434\n",
      "epoch 64 loss 3.0092607629156785\n",
      "epoch 65 loss 3.0090818001315336\n",
      "epoch 66 loss 3.0088884398163205\n",
      "epoch 67 loss 3.008677470438126\n",
      "epoch 68 loss 3.0083550883944468\n",
      "epoch 69 loss 3.0081890118626897\n",
      "epoch 70 loss 3.007746735820294\n",
      "epoch 71 loss 3.007655128464507\n",
      "epoch 72 loss 3.0072436291283555\n",
      "epoch 73 loss 3.006871256326102\n",
      "epoch 74 loss 3.0067477032105385\n",
      "epoch 75 loss 3.0066211844888495\n",
      "epoch 76 loss 3.0064696121422916\n",
      "epoch 77 loss 3.0061851383938203\n",
      "epoch 78 loss 3.00604283253093\n",
      "epoch 79 loss 3.0056604552087776\n",
      "epoch 80 loss 3.0055411320167567\n",
      "epoch 81 loss 3.0053202186940675\n",
      "epoch 82 loss 3.0052665866806247\n",
      "epoch 83 loss 3.0048709014080766\n",
      "epoch 84 loss 3.004836801338403\n",
      "epoch 85 loss 3.004586903720156\n",
      "epoch 86 loss 3.004334469959868\n",
      "epoch 87 loss 3.004237503234002\n",
      "epoch 88 loss 3.0042777152066638\n",
      "epoch 89 loss 3.0039498637734745\n",
      "epoch 90 loss 3.003547222425314\n",
      "epoch 91 loss 3.003283634506789\n",
      "epoch 92 loss 3.0031914089712335\n",
      "epoch 93 loss 3.003286451780835\n",
      "epoch 94 loss 3.0029412511893905\n",
      "epoch 95 loss 3.002735478611386\n",
      "epoch 96 loss 3.0026403402790316\n",
      "epoch 97 loss 3.00246653520582\n",
      "epoch 98 loss 3.0020148798127644\n",
      "epoch 99 loss 3.0021352260560605\n",
      "epoch 100 loss 3.001974520543499\n"
     ]
    }
   ],
   "source": [
    "model.train()  \n",
    "for i in range(100):\n",
    "    tloss = 0.\n",
    "    for idx, (x,y) in enumerate(loader):\n",
    "        x,y=x.to(device),y.to(device)\n",
    "        output = model(x)\n",
    "        output_flat = output.reshape(-1, ntokens)\n",
    "        loss = loss_func(output_flat, y.reshape(-1))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(),1)\n",
    "        optimizer.step()\n",
    "        tloss += loss.item()\n",
    "    print(f'epoch {i+1} loss {tloss/(idx+1)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f7afb2",
   "metadata": {},
   "source": [
    "If you are using GPU, it takes half an hour or so to train. If you use CPU only, it may take several hours, depending on your hardware. \n",
    "\n",
    "Next, we save the model on the local computer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d9effa9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),\"files/ch14/txtTrans.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441286c7",
   "metadata": {},
   "source": [
    "# 4. Use the Trained Model to Generate Text\n",
    "We can use the trained model to generate text. We first define the following sample() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "252eb61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def sample(model, prompt=\"The old man and the\", length=250, top_k=40):\n",
    "    model.eval()\n",
    "    idx=[]\n",
    "    for w in prompt.lower().split():\n",
    "        idx += vocabulary(tokenizer(w))\n",
    "    xs = torch.tensor(idx).unsqueeze(0)   \n",
    "    xs = xs.to(device)\n",
    "    x=xs\n",
    "    for i in range(0, length):\n",
    "        if x.size(1)>200:x=x[:,-200:]\n",
    "        logits = model(x)\n",
    "        logits = logits[:, -1, :]\n",
    "        v,_=torch.topk(logits,top_k)\n",
    "        logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        idx_next = torch.multinomial(probs, num_samples=1)\n",
    "        x = torch.cat((x, idx_next), dim=1)\n",
    "        xs = torch.cat((xs, idx_next), dim=1)        \n",
    "    txt=[vocabulary.lookup_token(i) for i in xs[0]]\n",
    "    return \" \".join(txt) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34f4811",
   "metadata": {},
   "source": [
    "In the *sample()* function, we give a prompt so the function know where to start. The default prompt is \"The old man and the\". You can also specify the length of the text you want to generate and the default length is 250 words. The function then uses the trained model to predict the next word based on the existing text. It then adds the predicted word to the text. The function repeats the process until the text reaches the desired length. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c148f7c9",
   "metadata": {},
   "source": [
    "We then reload the trained model as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4f972d36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"files/ch14/txtTrans.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bcaae3",
   "metadata": {},
   "source": [
    "Let's generate a passage with the model by using \"The old man and the\" as the prompt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1bbd71d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the old man and the old man ' ll fight is a cramp , he could not find him with coast and i ' s shivering increased as though i think about him and hard pull him though he could bring any more lions on the line was an accident . the line showed glowing below the line that were the first . they came guickly and they were the boy . it well and , and i did not much betting and the old man was . it . when he was no way slowly through his jaws . he saw the old man and the skiff forever . the skiff and his arm , he is my house where all colds and he had come in the terrace and he was no fear both badly but there was going more and he said . he is what you have . i ' s shack was more noble thing it , he had better and the surface at an airplane until something to take you sleep and would like to fight them as though , and the mast up the shark tore the boy had settled himself to look into the skiff . then he thought . make a beer cans and a piece and the skiff , the old man knew he was almost imperceptible . i know many years old man , he thought . and the surface only heard of the dark as the same spot where he leaned the old\n"
     ]
    }
   ],
   "source": [
    "print(sample(model, prompt=\"The old man and the\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9bc75d",
   "metadata": {},
   "source": [
    "Since we have trained the transformer with a document with less than 30,000 words in it, the model can only capture the statistical patterns of words in this small document. The text generated above does have the style of the training text. The main purpose of this chapter is for you to learn how to create a transformer and use data to train it from scratch. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
